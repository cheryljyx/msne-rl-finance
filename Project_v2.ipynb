{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import operator\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MP Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess(object):\n",
    "    \n",
    "    def __init__(self,transition_proba):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Process.\n",
    "        \n",
    "        Three attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \n",
    "        stationary_dist: dictionary of float\n",
    "            {state: proba} to represent the long-run probability of being in each state\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.stationary_dist = None\n",
    "    \n",
    "    def calculate_stationary_distribution(self):\n",
    "        # probability dictionary to matrix\n",
    "        num = len(self.states)\n",
    "        T = np.empty([num,num])\n",
    "        i = 0\n",
    "        for state, proba in self.transition_proba.items():\n",
    "            T[i,:] = list(proba.values())\n",
    "            i = i+1\n",
    "        \"\"\"\n",
    "        Stationary distribution is given by solving\n",
    "        pi = pi*T.\n",
    "        Solve for the left (row) eigen vector of transition matrix\n",
    "        \"\"\"      \n",
    "        value, left, right = scipy.linalg.eig(T, left=True,right=True)\n",
    "        eig_vec = left[:,np.where(np.abs(value-1)<1e-8)[0][0]].astype(float)\n",
    "        eig_vec = eig_vec/np.sum(eig_vec) # normalize\n",
    "        self.stationary_dist = {}\n",
    "        for i in range(num):\n",
    "            self.stationary_dist[self.states[i]] = eig_vec[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_0 = {'Facebook':{'Facebook':0.9,'Class 1':0.1,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 1':{'Facebook':0.5,'Class 1':0,'Class 2':0.5,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 2':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0.8,'Pass':0,'Sleep':0.2,'Pub':0},\n",
    "     'Class 3':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0.6,'Sleep':0,'Pub':0.4},\n",
    "     'Pass':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':1,'Pub':0},\n",
    "     'Sleep':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':1,'Pub':0},\n",
    "     'Pub':{'Facebook':0,'Class 1':0.2,'Class 2':0.4,'Class 3':0.4,'Pass':0,'Sleep':0,'Pub':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state space of the student MP problem:\n",
      "['Facebook', 'Class 1', 'Class 2', 'Class 3', 'Pass', 'Sleep', 'Pub']\n",
      "\n",
      "\n",
      "The stationary distribution of the student MP problem:\n",
      "{'Facebook': 0.0, 'Class 1': 0.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Pass': 0.0, 'Sleep': 1.0, 'Pub': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "mp = MarkovProcess(T_0)\n",
    "print('The state space of the student MP problem:')\n",
    "print(mp.states)\n",
    "print('\\n')\n",
    "print('The stationary distribution of the student MP problem:')\n",
    "mp.calculate_stationary_distribution()\n",
    "print(mp.stationary_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MRP Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess(object):\n",
    "    \n",
    "    def __init__(self,transition_proba,reward,gamma):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovRewardProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            (Reward) Process.\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "            {state : {state : reward}} representing the immediate reward\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Reward Process.\n",
    "        \n",
    "        gamma: float\n",
    "            a float between 0 and 1 to discount long-run rewards\n",
    "            \n",
    "        Five attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "        \n",
    "        gamma: float\n",
    "        \n",
    "        values: dictionary of float\n",
    "            {state: value} to represent the optimal value function of each state\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.values = None\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"\n",
    "        Check whether each state is self-absorbing\n",
    "        \n",
    "        return a dictionary of bools\n",
    "        \"\"\"\n",
    "        is_terminal_dict = {s: t[s] == 1 for s,t in self.transition_proba.items()}\n",
    "        return is_terminal_dict\n",
    "            \n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        A helper function to transform the transition dictionary to a 2-D matrix\n",
    "        \"\"\"\n",
    "        num = len(self.states)\n",
    "        T = np.empty([num,num])\n",
    "        for i in range(num):\n",
    "            for j in range(num):\n",
    "                T[i,j] = self.transition_proba[self.states[i]][self.states[j]]\n",
    "        return T\n",
    "    \n",
    "    def get_expected_reward(self):\n",
    "        \"\"\"\n",
    "        Transform the r(s,s') representation to R(s) = E[r(s,s')] representation\n",
    "        Return a dictionary {state: reward}\n",
    "        \"\"\"\n",
    "        T = self.get_transition_matrix()\n",
    "        num = len(self.states)\n",
    "        R = np.empty([num,num])\n",
    "        for i in range(num):\n",
    "            for j in range(num):\n",
    "                R[i,j] = self.reward[self.states[i]][self.states[j]]        \n",
    "        R_vec = np.diagonal(np.dot(T,R.T))\n",
    "        R_dict = {}\n",
    "        for i in range(num):\n",
    "            R_dict[self.states[i]] = R_vec[i]\n",
    "        return R_dict\n",
    "    \n",
    "    def get_value_function(self):\n",
    "        \"\"\"\n",
    "        Solve Bellman Equations \n",
    "            u = R + T*u\n",
    "        s.t. the terminal conditions\n",
    "        \n",
    "        Return a dictionary {state:value}\n",
    "        \"\"\"\n",
    "        R_dict = self.get_expected_reward()\n",
    "        is_terminal_dict = self.is_terminal()\n",
    "        R = np.empty(len(self.states))\n",
    "        is_terminal = list()\n",
    "        for i in range(len(self.states)):\n",
    "            R[i] = R_dict[self.states[i]]\n",
    "            is_terminal.append(is_terminal_dict[self.states[i]])\n",
    "        is_terminal = np.array(is_terminal)\n",
    "        T = self.get_transition_matrix()\n",
    "        v_vec = np.empty(len(self.states))\n",
    "        # terminal states\n",
    "        v_vec[is_terminal] = R[is_terminal]\n",
    "        # solve bellman equations for non-terminal states\n",
    "        v_recur = v_vec[~is_terminal]\n",
    "        R_inter = R[~is_terminal]+ np.dot(T[:,is_terminal][~is_terminal],v_vec[is_terminal])\n",
    "        v_recur = np.linalg.solve(self.gamma*T[:,~is_terminal][~is_terminal]-np.eye(T[:,~is_terminal][~is_terminal].shape[0]),-R_inter)\n",
    "        v_vec[~is_terminal] = v_recur\n",
    "        self.values={}\n",
    "        for i in range(len(self.states)):\n",
    "            self.values[self.states[i]] = v_vec[i]\n",
    "        return self.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_0 = {'Facebook':{'Facebook':-1,'Class 1':-1,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 1':{'Facebook':-2,'Class 1':0,'Class 2':-2,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 2':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':-2,'Pass':0,'Sleep':-2,'Pub':0},\n",
    "     'Class 3':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':-2,'Sleep':0,'Pub':-2},\n",
    "     'Pass':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':10,'Pub':0},\n",
    "     'Sleep':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Pub':{'Facebook':0,'Class 1':1,'Class 2':1,'Class 3':1,'Pass':0,'Sleep':0,'Pub':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expectation of immediate reward of each state is\n",
      "{'Facebook': -1.0, 'Class 1': -2.0, 'Class 2': -2.0, 'Class 3': -2.0, 'Pass': 10.0, 'Sleep': 0.0, 'Pub': 1.0}\n",
      "\n",
      "\n",
      "The long-run value of each state is\n",
      "{'Facebook': -7.637608431059512, 'Class 1': -5.0127289100145225, 'Class 2': 0.9426552976939073, 'Class 3': 4.087021246797093, 'Pass': 10.0, 'Sleep': 0.0, 'Pub': 1.9083923522141464}\n"
     ]
    }
   ],
   "source": [
    "my_mrp = MarkovRewardProcess(T_0,R_0,0.9)\n",
    "print('The expectation of immediate reward of each state is')\n",
    "print(my_mrp.get_expected_reward())\n",
    "print('\\n')\n",
    "print('The long-run value of each state is')\n",
    "print(my_mrp.get_value_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MDP Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: smoov & curly's bogus journey\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLw0qyFP7t4IYuI9XcDIGsARfO1QKMrpbN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = {}\n",
    "R = {}\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if not(i==1 and j == 2):\n",
    "            T_state = {}\n",
    "            R_state = {}\n",
    "            for direction in ['Up','Right','Down','Left']:\n",
    "                T_state_action = {}\n",
    "                R_state_action = {}\n",
    "                for m in range(4):\n",
    "                    for n in range(4):\n",
    "                        if not(m==1 and n == 2):\n",
    "                            T_state_action[(m,n)] = 0\n",
    "                            if i==3 and j == 3: R_state_action[(m,n)] = 10\n",
    "                            elif i == 3 and j == 2: R_state_action[(m,n)] = -10\n",
    "                            else: R_state_action[(m,n)] = -1\n",
    "                T_state[direction] = T_state_action\n",
    "                R_state[direction] = R_state_action\n",
    "            T[(i,j)] = T_state\n",
    "            R[(i,j)] = R_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T[(0,0)]['Up'][(0,1)] = 0.8\n",
    "T[(0,0)]['Up'][(1,0)] = 0.1\n",
    "T[(0,0)]['Up'][(0,0)] = 0.1\n",
    "T[(0,0)]['Right'][(0,1)] = 0.1\n",
    "T[(0,0)]['Right'][(1,0)] = 0.8\n",
    "T[(0,0)]['Right'][(0,0)] = 0.1\n",
    "T[(0,0)]['Left'][(0,0)] = 0.9\n",
    "T[(0,0)]['Left'][(0,1)] = 0.1\n",
    "T[(0,0)]['Down'][(0,1)] = 0.1\n",
    "T[(0,0)]['Down'][(0,0)] = 0.9\n",
    "\n",
    "T[(0,1)]['Up'][(0,2)] = 0.8\n",
    "T[(0,1)]['Up'][(0,1)] = 0.1\n",
    "T[(0,1)]['Up'][(1,1)] = 0.1\n",
    "T[(0,1)]['Right'][(1,1)] = 0.8\n",
    "T[(0,1)]['Right'][(0,2)] = 0.1\n",
    "T[(0,1)]['Right'][(0,0)] = 0.1\n",
    "T[(0,1)]['Left'][(0,1)] = 0.8\n",
    "T[(0,1)]['Left'][(0,0)] = 0.1\n",
    "T[(0,1)]['Left'][(0,2)] = 0.1\n",
    "T[(0,1)]['Down'][(0,0)] = 0.8\n",
    "T[(0,1)]['Down'][(0,1)] = 0.1\n",
    "T[(0,1)]['Down'][(1,1)] = 0.1\n",
    "\n",
    "T[(0,2)]['Up'][(0,3)] = 0.8\n",
    "T[(0,2)]['Up'][(0,2)] = 0.2\n",
    "T[(0,2)]['Right'][(0,2)] = 0.8\n",
    "T[(0,2)]['Right'][(0,3)] = 0.1\n",
    "T[(0,2)]['Right'][(0,1)] = 0.1\n",
    "T[(0,2)]['Left'][(0,2)] = 0.8\n",
    "T[(0,2)]['Left'][(0,3)] = 0.1\n",
    "T[(0,2)]['Left'][(0,1)] = 0.1\n",
    "T[(0,2)]['Down'][(0,1)] = 0.8\n",
    "T[(0,2)]['Down'][(0,2)] = 0.2\n",
    "\n",
    "T[(0,3)]['Up'][(0,3)] = 0.9\n",
    "T[(0,3)]['Up'][(1,3)] = 0.1\n",
    "T[(0,3)]['Right'][(1,3)] = 0.8\n",
    "T[(0,3)]['Right'][(0,3)] = 0.1\n",
    "T[(0,3)]['Right'][(0,2)] = 0.1\n",
    "T[(0,3)]['Left'][(0,3)] = 0.9\n",
    "T[(0,3)]['Left'][(0,2)] = 0.1\n",
    "T[(0,3)]['Down'][(0,2)] = 0.8\n",
    "T[(0,3)]['Down'][(0,3)] = 0.1\n",
    "T[(0,3)]['Down'][(1,3)] = 0.1\n",
    "\n",
    "T[(1,0)]['Up'][(1,1)] = 0.8\n",
    "T[(1,0)]['Up'][(0,0)] = 0.1\n",
    "T[(1,0)]['Up'][(2,0)] = 0.1\n",
    "T[(1,0)]['Right'][(2,0)] = 0.8\n",
    "T[(1,0)]['Right'][(1,0)] = 0.1\n",
    "T[(1,0)]['Right'][(1,1)] = 0.1\n",
    "T[(1,0)]['Left'][(0,0)] = 0.8\n",
    "T[(1,0)]['Left'][(1,0)] = 0.1\n",
    "T[(1,0)]['Left'][(1,1)] = 0.1\n",
    "T[(1,0)]['Down'][(1,0)] = 0.8\n",
    "T[(1,0)]['Down'][(2,0)] = 0.1\n",
    "T[(1,0)]['Down'][(0,0)] = 0.1\n",
    "\n",
    "T[(1,1)]['Up'][(1,1)] = 0.8\n",
    "T[(1,1)]['Up'][(0,1)] = 0.1\n",
    "T[(1,1)]['Up'][(2,1)] = 0.1\n",
    "T[(1,1)]['Right'][(2,1)] = 0.8\n",
    "T[(1,1)]['Right'][(1,1)] = 0.1\n",
    "T[(1,1)]['Right'][(1,0)] = 0.1\n",
    "T[(1,1)]['Left'][(0,1)] = 0.8\n",
    "T[(1,1)]['Left'][(1,0)] = 0.1\n",
    "T[(1,1)]['Left'][(1,1)] = 0.1\n",
    "T[(1,1)]['Down'][(1,0)] = 0.8\n",
    "T[(1,1)]['Down'][(2,1)] = 0.1\n",
    "T[(1,1)]['Down'][(0,1)] = 0.1\n",
    "\n",
    "T[(1,3)]['Up'][(1,3)] = 0.8\n",
    "T[(1,3)]['Up'][(0,3)] = 0.1\n",
    "T[(1,3)]['Up'][(2,3)] = 0.1\n",
    "T[(1,3)]['Right'][(2,3)] = 0.8\n",
    "T[(1,3)]['Right'][(1,3)] = 0.2\n",
    "T[(1,3)]['Left'][(0,3)] = 0.8\n",
    "T[(1,3)]['Left'][(1,3)] = 0.2\n",
    "T[(1,3)]['Down'][(1,3)] = 0.8\n",
    "T[(1,3)]['Down'][(0,3)] = 0.1\n",
    "T[(1,3)]['Down'][(2,3)] = 0.1\n",
    "\n",
    "T[(2,0)]['Up'][(2,1)] = 0.8\n",
    "T[(2,0)]['Up'][(1,0)] = 0.1\n",
    "T[(2,0)]['Up'][(3,0)] = 0.1\n",
    "T[(2,0)]['Right'][(3,0)] = 0.8\n",
    "T[(2,0)]['Right'][(2,0)] = 0.1\n",
    "T[(2,0)]['Right'][(2,1)] = 0.1\n",
    "T[(2,0)]['Left'][(1,0)] = 0.8\n",
    "T[(2,0)]['Left'][(2,0)] = 0.1\n",
    "T[(2,0)]['Left'][(2,1)] = 0.1\n",
    "T[(2,0)]['Down'][(1,0)] = 0.1\n",
    "T[(2,0)]['Down'][(2,0)] = 0.8\n",
    "T[(2,0)]['Down'][(3,0)] = 0.1\n",
    "\n",
    "T[(2,1)]['Up'][(2,2)] = 0.8\n",
    "T[(2,1)]['Up'][(1,1)] = 0.1\n",
    "T[(2,1)]['Up'][(3,1)] = 0.1\n",
    "T[(2,1)]['Right'][(3,1)] = 0.8\n",
    "T[(2,1)]['Right'][(2,0)] = 0.1\n",
    "T[(2,1)]['Right'][(2,2)] = 0.1\n",
    "T[(2,1)]['Left'][(1,1)] = 0.8\n",
    "T[(2,1)]['Left'][(2,0)] = 0.1\n",
    "T[(2,1)]['Left'][(2,2)] = 0.1\n",
    "T[(2,1)]['Down'][(1,1)] = 0.1\n",
    "T[(2,1)]['Down'][(2,0)] = 0.8\n",
    "T[(2,1)]['Down'][(3,1)] = 0.1\n",
    "\n",
    "T[(2,2)]['Up'][(2,3)] = 0.8\n",
    "T[(2,2)]['Up'][(2,2)] = 0.1\n",
    "T[(2,2)]['Up'][(3,2)] = 0.1\n",
    "T[(2,2)]['Right'][(3,1)] = 0.8\n",
    "T[(2,2)]['Right'][(2,0)] = 0.1\n",
    "T[(2,2)]['Right'][(2,2)] = 0.1\n",
    "T[(2,2)]['Left'][(1,1)] = 0.8\n",
    "T[(2,2)]['Left'][(2,0)] = 0.1\n",
    "T[(2,2)]['Left'][(2,2)] = 0.1\n",
    "T[(2,2)]['Down'][(2,2)] = 0.1\n",
    "T[(2,2)]['Down'][(2,1)] = 0.8\n",
    "T[(2,2)]['Down'][(3,2)] = 0.1\n",
    "\n",
    "T[(2,3)]['Up'][(2,3)] = 0.8\n",
    "T[(2,3)]['Up'][(1,3)] = 0.1\n",
    "T[(2,3)]['Up'][(3,3)] = 0.1\n",
    "T[(2,3)]['Right'][(3,3)] = 0.8\n",
    "T[(2,3)]['Right'][(2,3)] = 0.1\n",
    "T[(2,3)]['Right'][(2,2)] = 0.1\n",
    "T[(2,3)]['Left'][(1,3)] = 0.8\n",
    "T[(2,3)]['Left'][(2,2)] = 0.1\n",
    "T[(2,3)]['Left'][(2,3)] = 0.1\n",
    "T[(2,3)]['Down'][(1,3)] = 0.1\n",
    "T[(2,3)]['Down'][(2,2)] = 0.8\n",
    "T[(2,3)]['Down'][(3,3)] = 0.1\n",
    "\n",
    "T[(3,0)]['Up'][(3,1)] = 0.8\n",
    "T[(3,0)]['Up'][(2,0)] = 0.1\n",
    "T[(3,0)]['Up'][(3,0)] = 0.1\n",
    "T[(3,0)]['Right'][(3,0)] = 0.9\n",
    "T[(3,0)]['Right'][(3,1)] = 0.1\n",
    "T[(3,0)]['Left'][(2,0)] = 0.8\n",
    "T[(3,0)]['Left'][(3,0)] = 0.1\n",
    "T[(3,0)]['Left'][(3,1)] = 0.1\n",
    "T[(3,0)]['Down'][(3,0)] = 0.9\n",
    "T[(3,0)]['Down'][(2,0)] = 0.1\n",
    "\n",
    "T[(3,1)]['Up'][(3,2)] = 0.8\n",
    "T[(3,1)]['Up'][(2,1)] = 0.1\n",
    "T[(3,1)]['Up'][(3,1)] = 0.1\n",
    "T[(3,1)]['Right'][(3,1)] = 0.8\n",
    "T[(3,1)]['Right'][(3,0)] = 0.1\n",
    "T[(3,1)]['Right'][(3,2)] = 0.1\n",
    "T[(3,1)]['Left'][(2,1)] = 0.8\n",
    "T[(3,1)]['Left'][(3,0)] = 0.1\n",
    "T[(3,1)]['Left'][(3,2)] = 0.1\n",
    "T[(3,1)]['Down'][(3,0)] = 0.8\n",
    "T[(3,1)]['Down'][(2,1)] = 0.1\n",
    "T[(3,1)]['Down'][(3,1)] = 0.1\n",
    "\n",
    "for direction in ['Up','Right','Down','Left']:\n",
    "    T[(3,2)][direction][(3,2)] = 1\n",
    "    T[(3,3)][direction][(3,3)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess(object):\n",
    "        def __init__(self,transition_proba,reward,gamma,tol):\n",
    "            \"\"\"\n",
    "            Initialize the MarkovDecisionProcess instance.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            transition_proba: dictionary of dictionary of dictionary\n",
    "                {state : {action: {state : probability}}} representing the probabilities\n",
    "                of transitioning from one state to another in the Markov \n",
    "                Decision Process.\n",
    "\n",
    "            reward: dictionary of dictionary of dictionary\n",
    "                {state : {action: {state : reward}}} representing the immediate reward\n",
    "                of transitioning from one state to another in the Markov \n",
    "                Decision Process.\n",
    "\n",
    "            gamma: float\n",
    "                a float between 0 and 1 to discount long-run rewards\n",
    "            \n",
    "            tol: float\n",
    "                the tolerance level of error in iteration\n",
    "                \n",
    "            Eight attributes\n",
    "            ----------\n",
    "            transition_proba\n",
    "\n",
    "            states: vector\n",
    "                a vector to represent the state space of the Markov Process\n",
    "    \n",
    "            actions: vector\n",
    "                a vector to represent the action space of the Markov\n",
    "            \n",
    "            reward\n",
    "\n",
    "            gamma\n",
    "            \n",
    "            tol\n",
    "            \n",
    "            policy: dictionary of dictionary\n",
    "                {state: {action: proba}} to represent the stochastic policy\n",
    "            \n",
    "            values: dictionary of float\n",
    "                {state: value} to represent the optimal value function of each state\n",
    "            \"\"\"\n",
    "            self.transition_proba = transition_proba\n",
    "            self.reward = reward\n",
    "            self.gamma = gamma\n",
    "            self.tol = tol\n",
    "            self.states = list(transition_proba.keys())\n",
    "            self.actions = list(transition_proba[self.states[0]].keys())\n",
    "\n",
    "            self.policy = None\n",
    "            self.values = None\n",
    "\n",
    "        \n",
    "        def dict_to_matrix_1(self,dictionary):        \n",
    "            M = np.empty([len(self.states),len(self.actions),len(self.states)])\n",
    "            for i in range(len(self.states)):\n",
    "                for a in range(len(self.actions)):\n",
    "                    for j in range(len(self.states)):\n",
    "                        M[i][a][j]= dictionary[self.states[i]][self.actions[a]][self.states[j]]\n",
    "            return M\n",
    "        \n",
    "        def dict_to_matrix_2(self,dictionary):\n",
    "            M = np.empty([len(self.states),len(self.actions)])\n",
    "            for i in range(len(self.states)):\n",
    "                for a in range(len(self.actions)):\n",
    "                    M[i][a]= dictionary[self.states[i]][self.actions[a]]\n",
    "            return M\n",
    "        \n",
    "        def get_expected_reward(self):\n",
    "            \"\"\"\n",
    "            Calculate the expected reward per (state,action)\n",
    "            \n",
    "            Return {state: {action: reward}}\n",
    "            \"\"\"  \n",
    "            R = self.dict_to_matrix_1(self.reward)\n",
    "            T = self.dict_to_matrix_1(self.transition_proba)\n",
    "            R_expected_dict = {}\n",
    "            for i in range(len(self.states)):\n",
    "                R_state_reward_vec = np.diagonal(np.dot(T[i][:][:],(R[i][:][:]).T))\n",
    "                R_state_reward = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    R_state_reward[self.actions[a]] = R_state_reward_vec[a]\n",
    "                R_expected_dict[self.states[i]]= R_state_reward\n",
    "            return R_expected_dict\n",
    "        \n",
    "        def to_MRP(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, represent the problem as a Markov Reward Process\n",
    "            \"\"\"       \n",
    "            num = len(self.states)\n",
    "            \n",
    "            T_dict = {}\n",
    "            T_matrix = self.dict_to_matrix_1(self.transition_proba)\n",
    "            for i in range(num):\n",
    "                T_state_vec = np.zeros(num)\n",
    "                T_state_dict = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    T_state_vec += policy_dict[self.states[i]][self.actions[a]]*T_matrix[i][a][:]\n",
    "                for j in range(num):\n",
    "                    T_state_dict[self.states[j]] = T_state_vec[j]\n",
    "                T_dict[self.states[i]] = T_state_dict\n",
    "            \n",
    "            R_dict = {}\n",
    "            R_matrix = self.dict_to_matrix_1(self.reward)\n",
    "            for i in range(num):\n",
    "                R_state_vec = np.zeros(num)\n",
    "                R_state_dict = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    R_state_vec += policy_dict[self.states[i]][self.actions[a]]*R_matrix[i][a][:]\n",
    "                for j in range(num):\n",
    "                    R_state_dict[self.states[j]] = R_state_vec[j]\n",
    "                R_dict[self.states[i]] = R_state_dict            \n",
    "            \n",
    "            return MarkovRewardProcess(T_dict,R_dict,self.gamma)\n",
    "        \n",
    "        def is_terminal(self,s,a):\n",
    "            return self.transition_proba[s][a][s] == 1\n",
    "        \n",
    "        def get_state_value_function(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, return the value function of the corresponding MRP problem\n",
    "            \n",
    "            Return {state: value}\n",
    "            \"\"\"                 \n",
    "            mrp = self.to_MRP(policy_dict)\n",
    "            values = mrp.get_value_function()\n",
    "            return values\n",
    "        \n",
    "        def get_state_act_value_function(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, return the action value function of the corresponding MRP problem\n",
    "            \n",
    "            Return {state: {action: value}}\n",
    "            \"\"\" \n",
    "            v_dict = self.get_state_value_function(policy_dict)\n",
    "            v_vec = np.empty(len(self.states))\n",
    "            for i in range(len(self.states)):\n",
    "                v_vec[i] = v_dict[self.states[i]]\n",
    "                \n",
    "            R_expected_dict = self.get_expected_reward()\n",
    "            R_matrix = self.dict_to_matrix_2(R_expected_dict)\n",
    "                    \n",
    "            T = self.dict_to_matrix_1(self.transition_proba)\n",
    "            \n",
    "            q = {}\n",
    "            q_matrix = R_matrix + np.dot(T,v_vec)\n",
    "            for i in range(len(self.states)):\n",
    "                q_state = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    q_state[self.actions[a]] = q_matrix[i][a]\n",
    "                q[self.states[i]] = q_state\n",
    "    \n",
    "            return q\n",
    "        \n",
    "        def get_improved_policy(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, find the best possible action for each state based\n",
    "            on the action value function\n",
    "            \"\"\" \n",
    "            q = self.get_state_act_value_function(policy_dict)\n",
    "            greedy_policy = {}\n",
    "            for state,action_value in q.items():\n",
    "                policy_state = {}\n",
    "                greedy_action = max(action_value.items(),key=operator.itemgetter(1))[0]\n",
    "                for action in self.actions:\n",
    "                    if action == greedy_action: policy_state[action] = 1\n",
    "                    else: policy_state[action] = 0\n",
    "                greedy_policy[state] = policy_state\n",
    "            return greedy_policy\n",
    "        \n",
    "        def policy_iteration(self):\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            # initialization\n",
    "            policy_dict = {s:{a:1./len(self.actions) for a in self.actions } for s in self.states }\n",
    "            v_max = self.get_state_value_function(policy_dict)\n",
    "            epsilon = 1000\n",
    "            while epsilon >= self.tol:\n",
    "                policy_dict = self.get_improved_policy(policy_dict)\n",
    "                v = self.get_state_value_function(policy_dict)\n",
    "                epsilon = max(abs(v[s] - v_max[s]) for s in v) \n",
    "                v_max = v\n",
    "            \n",
    "            return policy_dict\n",
    "        \n",
    "        def value_iteration(self,synchronous):\n",
    "            v = np.zeros(len(self.states))\n",
    "            T = self.dict_to_matrix_1(self.transition_proba)\n",
    "            R_matrix = self.dict_to_matrix_2(self.get_expected_reward())\n",
    "\n",
    "            iterate = True           \n",
    "            while iterate:\n",
    "                if synchronous:\n",
    "                    OldValues = v.copy()\n",
    "                    v = np.max(R_matrix+self.gamma*np.dot(T,OldValues),axis=1)\n",
    "            \n",
    "            self.values={}\n",
    "            for i in range(num_s):\n",
    "                self.values[self.states[i]] = v[i]\n",
    "        \n",
    "        \n",
    "        def generate_initial_state_action(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Use inversion method to simulate and return the initial state and corresponding action,\n",
    "            given some fixed policy\n",
    "            \n",
    "            Parameters: the current state the action\n",
    "            \n",
    "            Return: the initial state and action\n",
    "            \"\"\"\n",
    "            # uniformly generate a state\n",
    "            n = len(self.states)\n",
    "            u1 = np.random.uniform(0,1)\n",
    "            for i in range(n):\n",
    "                if i*1.0/n < u1 <= (i+1)*1.0/n: \n",
    "                    s = self.states[i]\n",
    "                    break\n",
    "            # sample the corresponding action given some fixed policy\n",
    "            u2 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action,proba in policy_dict[s].items():\n",
    "                cum_proba += proba\n",
    "                if u2 <= cum_proba:\n",
    "                    a = action\n",
    "                    break \n",
    "            return s,a\n",
    "        \n",
    "        def simulate_transition(self,s,a):\n",
    "            \"\"\"\n",
    "            Use inversion method to simulate and return the next state and the corresponding reward\n",
    "            \n",
    "            Parameters: the current state the action\n",
    "            \n",
    "            Return: the next sample state and reward\n",
    "            \"\"\"\n",
    "            u = np.random.uniform(0,1)\n",
    "            proba_dict = self.transition_proba[s][a]\n",
    "            cum_proba = 0\n",
    "            for i in range(len(self.states)):\n",
    "                cum_proba += proba_dict[self.states[i]]\n",
    "                if u <= cum_proba:\n",
    "                    s_new = self.states[i]\n",
    "                    break  \n",
    "            r = self.reward[s][a][s_new]\n",
    "            return s_new,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mdp = MarkovDecisionProcess(T,R,1,1)   # no discount, tol=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 0): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 1): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (2, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (3, 0): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 1): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (3, 3): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy = my_mdp.policy_iteration()\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model-Free Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the original Markov Decision Process under the optimal policy to generate episodes of observations (R,S,A), and estimate the value functions using Monte Carlo learning and Temporal Difference Learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function given by the original MDP under the optimal policy is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.71009019838139,\n",
       " (0, 1): 3.071932119775163,\n",
       " (0, 2): 4.449914383561644,\n",
       " (0, 3): 5.699914383561644,\n",
       " (1, 0): 0.8153548272312068,\n",
       " (1, 1): 2.0480740094833125,\n",
       " (1, 3): 7.106164383561644,\n",
       " (2, 0): 1.9112649294496935,\n",
       " (2, 1): 3.4521639072648256,\n",
       " (2, 2): 5.205479452054796,\n",
       " (2, 3): 8.356164383561644,\n",
       " (3, 0): 0.679983209147123,\n",
       " (3, 1): 0.8297294467265717,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_policy = optimal_policy\n",
    "mrp_pre = my_mdp.to_MRP(toy_policy)\n",
    "mdp_value_function = mrp_pre.get_value_function()\n",
    "mdp_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Monte Carlo Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monte_Carlo_learning(mdp,policy_dict,k):  \n",
    "    \"\"\"\n",
    "    The MC learning algorithm predicts the value functions of an MDP object given some fixed policy, after \n",
    "    k episodes\n",
    "    \n",
    "    Parameters: \n",
    "        mdp object: representing the environment the algorithm interacts with\n",
    "        policy_dict: a fixed policy to evaluate\n",
    "        k: the number of episodes\n",
    "        \n",
    "    Return: value functions\n",
    "    \"\"\"\n",
    "    n = len(mdp.states)\n",
    "    value_dict={}\n",
    "    count_dict={}\n",
    "    for state in mdp.states:\n",
    "        value_dict[state] = 0\n",
    "        count_dict[state] = 0\n",
    "            \n",
    "    # simulate k episodes\n",
    "    for j in range(k):\n",
    "        # simulate the initial state\n",
    "        s,a = mdp.generate_initial_state_action(policy_dict)\n",
    "        episode= [(0,s,a)]  \n",
    "        \n",
    "        # simulate the following transitions until hitting the terminal state\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            s, r = mdp.simulate_transition(s,a)\n",
    "            u2 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action,proba in policy_dict[s].items():\n",
    "                cum_proba += proba\n",
    "                if u2 <= cum_proba:\n",
    "                    a = action\n",
    "                    break \n",
    "            episode.append((r,s,a))\n",
    "\n",
    "        # update value function for each state that appears in the episode\n",
    "        for state in mdp.states:\n",
    "            t = -1\n",
    "            for j in range(len(episode)):\n",
    "                if episode[j][1] == state: \n",
    "                    t = j\n",
    "                    break\n",
    "            if t >= 0: # if the state appears in the current episode, we update its value function\n",
    "                if t == len(episode)-1:       # the terminal state\n",
    "                    G = 10 if episode[t][1] == (3,3) else -10\n",
    "                else:\n",
    "                    G = 0\n",
    "                    for j in range(t+1,len(episode)): G += episode[j][0]*mdp.gamma**(j-1-t)\n",
    "                    end_reward = 10 if episode[len(episode)-1][1] == (3,3) else -10\n",
    "                    G += end_reward*mdp.gamma**(len(episode)-1-t)\n",
    "                count_dict[state] += 1\n",
    "                value_dict[state] += 1.0/count_dict[state]*(G - value_dict[state])          \n",
    "        \n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted value functions by MC after 100,000 episodes is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.6344118085582204,\n",
       " (0, 1): 2.989469485440768,\n",
       " (0, 2): 4.393167499456858,\n",
       " (0, 3): 5.646228730366502,\n",
       " (1, 0): 0.7729044834308006,\n",
       " (1, 1): 2.0664186623265777,\n",
       " (1, 3): 7.064985669661557,\n",
       " (2, 0): 1.925476358503885,\n",
       " (2, 1): 3.46287265476734,\n",
       " (2, 2): 5.191906405830398,\n",
       " (2, 3): 8.338632254846726,\n",
       " (3, 0): 0.6835730507191533,\n",
       " (3, 1): 0.9828378261246438,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Monte_Carlo_learning(my_mdp,toy_policy,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Forward TD(0) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Temporal_Difference_learning(mdp,policy_dict,alpha,lambda_TD,n):\n",
    "    \"\"\"\n",
    "    Use TD(lambda) to predict the value functions of a given policy\n",
    "    \n",
    "    Parameters: \n",
    "        mdp object\n",
    "        given policy dictionary\n",
    "        step size alpha\n",
    "        lambda_TD\n",
    "        the minimum number of iterations n\n",
    "        \n",
    "    Return:\n",
    "        value function dictionary\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    value_dict = {}\n",
    "    # randomly initialize the value function following a standard normal distribution\n",
    "    for state in mdp.states: value_dict[state] = np.random.randn()+3\n",
    "    # terminal value\n",
    "    value_dict[(3,3)] = 10\n",
    "    value_dict[(3,2)] = -10\n",
    "    \n",
    "    # n controls the smallest number of observations to get a good estimation\n",
    "    while count < n:\n",
    "        count += 1\n",
    "        s,a = mdp.generate_initial_state_action(policy_dict)\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            count += 1\n",
    "            s_next, r = mdp.simulate_transition(s,a)\n",
    "            TD_target = r + mdp.gamma*value_dict[s_next] - value_dict[s]\n",
    "            value_dict[s] += alpha*TD_target\n",
    "            s = s_next\n",
    "            u2 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action,proba in policy_dict[s].items():\n",
    "                cum_proba += proba\n",
    "                if u2 <= cum_proba:\n",
    "                    a = action\n",
    "                    break \n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given step size alpha = 0.001, the predicted value functions by forward TD(0) after 100,000 time steps (results are unstable when the the number of iterations is small) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.0238378127503425,\n",
       " (0, 1): 1.8796396802413093,\n",
       " (0, 2): 3.7855300979426976,\n",
       " (0, 3): 5.503489729177209,\n",
       " (1, 0): 1.1141168913608221,\n",
       " (1, 1): 2.1086685456157688,\n",
       " (1, 3): 7.1214770811154,\n",
       " (2, 0): 2.066971194715635,\n",
       " (2, 1): 3.578692426924436,\n",
       " (2, 2): 5.246023287076667,\n",
       " (2, 3): 8.351350796532826,\n",
       " (3, 0): 1.0093431114984481,\n",
       " (3, 1): 0.9795103563825974,\n",
       " (3, 2): -10,\n",
       " (3, 3): 10}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temporal_Difference_learning(my_mdp,toy_policy,0.001,0,100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values are highly sensitive to the initial random initialization, even after 10000 time steps. Thus MC learning algorithm is better w.r.t. this MDP problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model-Free Control "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Sarsa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon):\n",
    "    \"\"\"\n",
    "    A helper function that return the epsilon-greedy policy dictionary, given all the action values of some state\n",
    "    \"\"\"\n",
    "    state_epsilon_greedy_policy = {}\n",
    "    max_action_value = -10000\n",
    "    max_action = \"\"\n",
    "    for action in mdp.actions:\n",
    "        state_epsilon_greedy_policy[action] = epsilon/l\n",
    "        if action_value_dict[action] > max_action_value:\n",
    "            max_action_value = action_value_dict[action]\n",
    "            max_action = action\n",
    "    state_epsilon_greedy_policy[max_action] += 1-epsilon\n",
    "    return state_epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(mdp,alpha,lambda_TD,n,epsilon):\n",
    "    \"\"\"\n",
    "    Use Sarsa algorithm to solve the optimal value function, and derive the corresponding optimal policy\n",
    "    \n",
    "    Parameters:\n",
    "        mdp object\n",
    "        step size alpha\n",
    "        lambda TD\n",
    "        the minimum number of iterations n\n",
    "        epsilon\n",
    "    \n",
    "    Return:\n",
    "        optimal value dictionary\n",
    "        epsilon greedy policy dictionary\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    state_action_value_dict = {}\n",
    "    epsilon_greedy_policy = {}\n",
    "    m = len(mdp.states)\n",
    "    l = len(mdp.actions)\n",
    "      \n",
    "    for state in mdp.states:\n",
    "        action_value_dict = {}\n",
    "        for action in mdp.actions:\n",
    "            # randomly initialize the value function following a standard normal distribution\n",
    "            if state!= (3,3) and state!=(3,2):\n",
    "                action_value_dict[action] = np.random.randn()+3\n",
    "            elif state == (3,3):\n",
    "                action_value_dict[action] = 10\n",
    "            else:\n",
    "                action_value_dict[action] = -10\n",
    "        # apply the epsilon-greedy policy improvement to the initial Q function \n",
    "        epsilon_greedy_policy[state] = epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon)\n",
    "        state_action_value_dict[state] = action_value_dict\n",
    "        \n",
    "    while count < n:\n",
    "        count += 1\n",
    "        s,a = mdp.generate_initial_state_action(epsilon_greedy_policy)\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            count += 1\n",
    "            s_next, r = mdp.simulate_transition(s,a)\n",
    "            # sample A' using the current epsilon-greedy policy\n",
    "            u1 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action in mdp.actions:\n",
    "                cum_proba += epsilon_greedy_policy[s_next][action]\n",
    "                if u1 <= cum_proba:\n",
    "                    a_next = action\n",
    "                    break\n",
    "            # update Q(S,A)\n",
    "            state_action_value_dict[s][a] += alpha*(r+mdp.gamma*state_action_value_dict[s_next][a_next] - state_action_value_dict[s][a])\n",
    "            # apply the epsilon-greedy policy improvement\n",
    "            epsilon_greedy_policy[s] = epsilon_greedy_policy_improvement(mdp,state_action_value_dict[s],l,epsilon)\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "    \n",
    "    # finally we extract the optimal action-value function for each state\n",
    "    state_value_dict = {}\n",
    "    for state in mdp.states:\n",
    "        max_action_value = -10000\n",
    "        for action in mdp.actions:\n",
    "            if state_action_value_dict[state][action]>max_action_value:\n",
    "                max_action_value = state_action_value_dict[state][action]\n",
    "        state_value_dict[state] = max_action_value\n",
    "    return state_value_dict, epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.6290125893626852,\n",
       " (0, 1): 3.105362444045345,\n",
       " (0, 2): 4.518068305020353,\n",
       " (0, 3): 5.752159212775516,\n",
       " (1, 0): 1.2007893310893003,\n",
       " (1, 1): 2.34575790707091,\n",
       " (1, 3): 7.1372594549448936,\n",
       " (2, 0): 2.179383044661068,\n",
       " (2, 1): 3.845989734765322,\n",
       " (2, 2): 5.600212160187006,\n",
       " (2, 3): 8.389643128062108,\n",
       " (3, 0): 0.8702483710270821,\n",
       " (3, 1): 0.9091863218607308,\n",
       " (3, 2): -10,\n",
       " (3, 3): 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_action_value_function, optimal_policy_sarsa = Sarsa(my_mdp,0.01,0,100000,0.001)\n",
    "optimal_action_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark MDP value function is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.71009019838139,\n",
       " (0, 1): 3.071932119775163,\n",
       " (0, 2): 4.449914383561644,\n",
       " (0, 3): 5.699914383561644,\n",
       " (1, 0): 0.8153548272312068,\n",
       " (1, 1): 2.0480740094833125,\n",
       " (1, 3): 7.106164383561644,\n",
       " (2, 0): 1.9112649294496935,\n",
       " (2, 1): 3.4521639072648256,\n",
       " (2, 2): 5.205479452054796,\n",
       " (2, 3): 8.356164383561644,\n",
       " (3, 0): 0.679983209147123,\n",
       " (3, 1): 0.8297294467265717,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action-value function is in general similar to the value iteration results of the underlying MDP. However, the derived optimal policy is slightly different than the MDP solution for 1/2 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (0, 1): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (0, 2): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (0, 3): {'Up': 0.00025, 'Right': 0.99925, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (1, 0): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (1, 1): {'Up': 0.00025, 'Right': 0.99925, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (1, 3): {'Up': 0.00025, 'Right': 0.99925, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (2, 0): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (2, 1): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (2, 2): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (2, 3): {'Up': 0.00025, 'Right': 0.99925, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (3, 0): {'Up': 0.00025, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.99925},\n",
       " (3, 1): {'Up': 0.00025, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.99925},\n",
       " (3, 2): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025},\n",
       " (3, 3): {'Up': 0.99925, 'Right': 0.00025, 'Down': 0.00025, 'Left': 0.00025}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark optimal policy of the underlying MDP is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 0): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 1): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (2, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (3, 0): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 1): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (3, 3): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0}}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we would like to learn the optimal policy while following an epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(mdp,alpha,lambda_TD,n,epsilon):\n",
    "    \"\"\"\n",
    "    Use Q-learning to learn the optimal value function\n",
    "    \n",
    "    Parameters:\n",
    "        mdp object\n",
    "        step size alpha\n",
    "        lambda TD\n",
    "        the minimum number of iterations n\n",
    "        epsilon\n",
    "    \n",
    "    Return:\n",
    "        optimal value function dictionary\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    state_action_value_dict = {}\n",
    "    epsilon_greedy_policy = {}\n",
    "    m = len(mdp.states)\n",
    "    l = len(mdp.actions)\n",
    "    \n",
    "    # we use the initial epsilon-greedy policy as the behavior policy\n",
    "    for state in mdp.states:\n",
    "        action_value_dict = {}\n",
    "        for action in mdp.actions:\n",
    "            # randomly initialize the value function following a standard normal distribution\n",
    "            if state!= (3,3) and state!=(3,2):\n",
    "                action_value_dict[action] = np.random.randn()+3\n",
    "            elif state == (3,3):\n",
    "                action_value_dict[action] = 10\n",
    "            else:\n",
    "                action_value_dict[action] = -10\n",
    "        # apply the epsilon-greedy policy improvement to the initial Q function \n",
    "        epsilon_greedy_policy[state] = epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon)\n",
    "        state_action_value_dict[state] = action_value_dict\n",
    "        \n",
    "    while count < n:\n",
    "        count += 1\n",
    "        s,a = mdp.generate_initial_state_action(epsilon_greedy_policy)\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            count += 1\n",
    "            s_next, r = mdp.simulate_transition(s,a)\n",
    "            max_action_value = -10000\n",
    "            for action in mdp.actions:\n",
    "                if state_action_value_dict[s_next][action]>max_action_value: max_action_value = state_action_value_dict[s_next][action]\n",
    "            # update Q(S,A) following the greedy policy pi\n",
    "            state_action_value_dict[s][a] += alpha*(r+mdp.gamma*max_action_value - state_action_value_dict[s][a])\n",
    "            # apply the epsilon-greedy policy improvement\n",
    "            epsilon_greedy_policy[s] = epsilon_greedy_policy_improvement(mdp,state_action_value_dict[s],l,epsilon)\n",
    "            s = s_next\n",
    "            # sample A using the epsilon-greedy policy\n",
    "            u1 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action in mdp.actions:\n",
    "                cum_proba += epsilon_greedy_policy[s][action]\n",
    "                if u1 <= cum_proba:\n",
    "                    a = action\n",
    "                    break\n",
    "\n",
    "    # finally we extract the optimal action-value function for each state\n",
    "    state_value_dict = {}\n",
    "    for state in mdp.states:\n",
    "        max_action_value = -10000\n",
    "        for action in mdp.actions:\n",
    "            if state_action_value_dict[state][action]>max_action_value:\n",
    "                max_action_value = state_action_value_dict[state][action]\n",
    "        state_value_dict[state] = max_action_value\n",
    "    return state_value_dict      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.6895757849754014,\n",
       " (0, 1): 3.1479838437774355,\n",
       " (0, 2): 4.433779538588049,\n",
       " (0, 3): 5.580674923400579,\n",
       " (1, 0): 1.1545773060535598,\n",
       " (1, 1): 2.0624480291476215,\n",
       " (1, 3): 7.238937574808567,\n",
       " (2, 0): 2.1223488419520122,\n",
       " (2, 1): 3.34380762845725,\n",
       " (2, 2): 4.924513303626981,\n",
       " (2, 3): 8.596780930433304,\n",
       " (3, 0): 1.007272783043343,\n",
       " (3, 1): 1.2570082465835943,\n",
       " (3, 2): -10,\n",
       " (3, 3): 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_learning(my_mdp,0.01,0,100000,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark MDP value function is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.71009019838139,\n",
       " (0, 1): 3.071932119775163,\n",
       " (0, 2): 4.449914383561644,\n",
       " (0, 3): 5.699914383561644,\n",
       " (1, 0): 0.8153548272312068,\n",
       " (1, 1): 2.0480740094833125,\n",
       " (1, 3): 7.106164383561644,\n",
       " (2, 0): 1.9112649294496935,\n",
       " (2, 1): 3.4521639072648256,\n",
       " (2, 2): 5.205479452054796,\n",
       " (2, 3): 8.356164383561644,\n",
       " (3, 0): 0.679983209147123,\n",
       " (3, 1): 0.8297294467265717,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Value function Approximation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regard to my previous MDP problem, I explore using a linear model to approximate the value functions. To start with a simple model, I use a naive feature representation of the state space, i.e. an one-hot-vector. Then I build a linear model to find the optimal weights and the corresponding action-value functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Monte Carlo prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (0, 1): array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (0, 2): array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (0, 3): array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (1, 0): array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (1, 1): array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (1, 3): array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (2, 0): array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " (2, 1): array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " (2, 2): array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " (2, 3): array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
       " (3, 0): array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
       " (3, 1): array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " (3, 2): array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " (3, 3): array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featue representation of states\n",
    "feature_state_matrix = np.eye(len(my_mdp.states),len(my_mdp.states))\n",
    "feature_state = {}\n",
    "i = 0\n",
    "for state in my_mdp.states:\n",
    "    feature_state[state] = feature_state_matrix[:,i]\n",
    "    i += 1\n",
    "feature_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monte_Carlo_value_func_approximation(mdp,feature_rep,policy_dict,k,alpha):\n",
    "    # random initialization of weights\n",
    "    n = len(mdp.states)\n",
    "    w = np.random.randn(n)\n",
    "      \n",
    "    # simulate k episodes    \n",
    "    for j in range(k):\n",
    "        # simulate the initial state\n",
    "        s,a = mdp.generate_initial_state_action(policy_dict)\n",
    "        episode= [(0,s,a)]  \n",
    "        \n",
    "        # simulate the following transitions until hitting the terminal state\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            s, r = mdp.simulate_transition(s,a)\n",
    "            u2 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action,proba in policy_dict[s].items():\n",
    "                cum_proba += proba\n",
    "                if u2 <= cum_proba:\n",
    "                    a = action\n",
    "                    break \n",
    "            episode.append((r,s,a))\n",
    "        \n",
    "        # prepare the \"training data\" (S_t, G_t)\n",
    "        T = len(episode)\n",
    "        G_vec = np.zeros(T)\n",
    "        for t in range(T):\n",
    "            G = 0\n",
    "            if t < T-1:\n",
    "                for j in range(t+1,T): G += episode[j][0]*mdp.gamma**(j-1-t)\n",
    "            end_reward = 10 if episode[T-1][1] == (3,3) else -10\n",
    "            G += end_reward*mdp.gamma**(T-1-t) \n",
    "            G_vec[t] = G\n",
    "        \n",
    "        # stochastic gradient descent     \n",
    "        perm = np.random.permutation(T)\n",
    "        for j in perm:\n",
    "            st = episode[j][1]\n",
    "            w += alpha*(G_vec[j] - np.dot(feature_rep[st],w))*feature_rep[st]\n",
    "    \n",
    "    # return the predicted value functions\n",
    "    value_dict = {}\n",
    "    for state in mdp.states:\n",
    "        value_dict[state] = np.dot(feature_rep[state],w)\n",
    "    \n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.5429151180014966,\n",
       " (0, 1): 3.003894839330005,\n",
       " (0, 2): 4.42200389599265,\n",
       " (0, 3): 5.731061326558239,\n",
       " (1, 0): 0.9696373702623957,\n",
       " (1, 1): 2.036426482014669,\n",
       " (1, 3): 7.199549233208328,\n",
       " (2, 0): 2.151298167513606,\n",
       " (2, 1): 3.3807116065603946,\n",
       " (2, 2): 5.124856998954625,\n",
       " (2, 3): 8.359090140023216,\n",
       " (3, 0): 0.7061300664306106,\n",
       " (3, 1): 1.015939499400649,\n",
       " (3, 2): -9.999994267863787,\n",
       " (3, 3): 9.999999999999112}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Monte_Carlo_value_func_approximation(my_mdp,feature_state,toy_policy,100000,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bench mark MDP optimal value function is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.71009019838139,\n",
       " (0, 1): 3.071932119775163,\n",
       " (0, 2): 4.449914383561644,\n",
       " (0, 3): 5.699914383561644,\n",
       " (1, 0): 0.8153548272312068,\n",
       " (1, 1): 2.0480740094833125,\n",
       " (1, 3): 7.106164383561644,\n",
       " (2, 0): 1.9112649294496935,\n",
       " (2, 1): 3.4521639072648256,\n",
       " (2, 2): 5.205479452054796,\n",
       " (2, 3): 8.356164383561644,\n",
       " (3, 0): 0.679983209147123,\n",
       " (3, 1): 0.8297294467265717,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted value functions by the MC algorithm is very similar to the MDP solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 TD(0) prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Temporal_Difference_value_func_approximation(mdp,feature_rep,policy_dict,alpha,lambda_TD,n):\n",
    "    # random initialization of weights\n",
    "    w = np.random.randn(len(mdp.states))\n",
    "    \n",
    "    # n controls the smallest number of observations to get a good estimation\n",
    "    count = 0\n",
    "    while count < n:\n",
    "        # sample an episode\n",
    "        count += 1\n",
    "        s,a = mdp.generate_initial_state_action(policy_dict)\n",
    "        episode = []\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            count += 1\n",
    "            s_next, r = mdp.simulate_transition(s,a)\n",
    "            episode.append((r,s,s_next))\n",
    "            s = s_next\n",
    "            u2 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action,proba in policy_dict[s].items():\n",
    "                cum_proba += proba\n",
    "                if u2 <= cum_proba:\n",
    "                    a = action\n",
    "                    break     \n",
    "        # stochastic gradient descent for data in the episode   \n",
    "        T = len(episode)\n",
    "        perm = np.random.permutation(T)\n",
    "        for j in perm:\n",
    "            s = episode[j][1]\n",
    "            s_next = episode[j][2]\n",
    "            r = episode[j][0]\n",
    "            if j == T-1:\n",
    "                next_value = 10 if s_next == (3,3) else -10\n",
    "            else:\n",
    "                next_value = np.dot(feature_rep[s_next],w)\n",
    "            TD_target = r + mdp.gamma*next_value - np.dot(feature_rep[s],w)                \n",
    "            w += alpha*TD_target*feature_rep[s]\n",
    "    \n",
    "    # return the predicted value functions\n",
    "    value_dict = {}\n",
    "    for state in mdp.states:\n",
    "        value_dict[state] = np.dot(feature_rep[state],w)\n",
    "    value_dict[(3,2)] = -10\n",
    "    value_dict[(3,3)] = 10\n",
    "    return value_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.7793095235761278,\n",
       " (0, 1): 3.0698271210640735,\n",
       " (0, 2): 4.461107635328946,\n",
       " (0, 3): 5.695309314930527,\n",
       " (1, 0): 0.7964916915753437,\n",
       " (1, 1): 1.9234302587006789,\n",
       " (1, 3): 7.063135039390187,\n",
       " (2, 0): 1.7922130535537446,\n",
       " (2, 1): 3.258505851343289,\n",
       " (2, 2): 4.971389016136814,\n",
       " (2, 3): 8.36150595599048,\n",
       " (3, 0): 0.6862559976098094,\n",
       " (3, 1): 0.830344174962392,\n",
       " (3, 2): -10,\n",
       " (3, 3): 10}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temporal_Difference_value_func_approximation(my_mdp,feature_state,toy_policy,0.001,0,1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bench mark MDP optimal value function is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.71009019838139,\n",
       " (0, 1): 3.071932119775163,\n",
       " (0, 2): 4.449914383561644,\n",
       " (0, 3): 5.699914383561644,\n",
       " (1, 0): 0.8153548272312068,\n",
       " (1, 1): 2.0480740094833125,\n",
       " (1, 3): 7.106164383561644,\n",
       " (2, 0): 1.9112649294496935,\n",
       " (2, 1): 3.4521639072648256,\n",
       " (2, 2): 5.205479452054796,\n",
       " (2, 3): 8.356164383561644,\n",
       " (3, 0): 0.679983209147123,\n",
       " (3, 1): 0.8297294467265717,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the TD(0)-based value approximation converges to the MDP optimal solution after 1,000,000 time steps. The algorithm yields unstable results after 100,000 time steps, which guarantees convergence of the tabular TD(0) algorithm. Therefore, it takes more time for the TD(0)-based value approximation algorithm to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Sarsa control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  0): {'Up': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (0,\n",
       "  1): {'Up': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (0,\n",
       "  2): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (0,\n",
       "  3): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (1,\n",
       "  0): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (1,\n",
       "  1): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (1,\n",
       "  3): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (2,\n",
       "  0): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (2,\n",
       "  1): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (2,\n",
       "  2): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (2,\n",
       "  3): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (3,\n",
       "  0): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (3,\n",
       "  1): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " (3,\n",
       "  2): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0.])},\n",
       " (3,\n",
       "  3): {'Up': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0.]), 'Right': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0.]), 'Down': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0.]), 'Left': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.])}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_state_action_matrix = np.eye(len(my_mdp.states)*len(my_mdp.actions))\n",
    "feature_state_action = {}\n",
    "i = 0\n",
    "for state in my_mdp.states:\n",
    "    feature_action = {}\n",
    "    for action in my_mdp.actions:\n",
    "        feature_action[action] = feature_state_action_matrix[:,i]\n",
    "        i += 1\n",
    "    feature_state_action[state] = feature_action\n",
    "feature_state_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa_value_func_approximation(mdp,feature_rep,alpha,lambda_TD,n,epsilon):\n",
    "    count = 0    \n",
    "    state_action_value_dict = {}\n",
    "    epsilon_greedy_policy = {}\n",
    "    m = len(mdp.states)\n",
    "    l = len(mdp.actions)\n",
    "    w = np.random.randn(m*l)\n",
    "      \n",
    "    for state in mdp.states:\n",
    "        action_value_dict = {}\n",
    "        for action in mdp.actions:\n",
    "            if state == (3,3):\n",
    "                action_value_dict[action] = 10\n",
    "            elif state == (3,2):\n",
    "                action_value_dict[action] = -10\n",
    "            else:\n",
    "                action_value_dict[action] = np.dot(feature_rep[state][action],w)\n",
    "        epsilon_greedy_policy[state] = epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon)\n",
    "        state_action_value_dict[state] = action_value_dict\n",
    "        \n",
    "    while count < n:\n",
    "        count += 1\n",
    "        s,a = mdp.generate_initial_state_action(epsilon_greedy_policy)\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            count += 1\n",
    "            s_next, r = mdp.simulate_transition(s,a)\n",
    "            # sample A' using the current epsilon-greedy policy\n",
    "            u1 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action in mdp.actions:\n",
    "                cum_proba += epsilon_greedy_policy[s_next][action]\n",
    "                if u1 <= cum_proba:\n",
    "                    a_next = action\n",
    "                    break\n",
    "            # update w\n",
    "            if mdp.is_terminal(s_next,a_next):\n",
    "                next_value = 10 if s_next == (3,3) else -10\n",
    "            else: \n",
    "                next_value = np.dot(feature_rep[s_next][a_next],w)\n",
    "            w += alpha*(r+mdp.gamma*next_value - np.dot(feature_rep[s][a],w))*feature_rep[s][a]\n",
    "            \n",
    "            # update Q(s,a) and apply the epsilon-greedy policy improvement\n",
    "            for state in mdp.states:\n",
    "                action_value_dict = {}\n",
    "                for action in mdp.actions:\n",
    "                    if state == (3,3):\n",
    "                        action_value_dict[action] = 10\n",
    "                    elif state == (3,2):\n",
    "                        action_value_dict[action] = -10\n",
    "                    else:\n",
    "                        action_value_dict[action] = np.dot(feature_rep[state][action],w)\n",
    "                epsilon_greedy_policy[state] = epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon)\n",
    "                state_action_value_dict[state] = action_value_dict\n",
    "            s = s_next\n",
    "            a = a_next\n",
    "    \n",
    "    # finally we extract the optimal action-value function for each state\n",
    "    state_value_dict = {}\n",
    "    for state in mdp.states:\n",
    "        max_action_value = -10000\n",
    "        for action in mdp.actions:\n",
    "            if state_action_value_dict[state][action]>max_action_value:\n",
    "                max_action_value = state_action_value_dict[state][action]\n",
    "        state_value_dict[state] = max_action_value\n",
    "        \n",
    "    return state_value_dict, epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_dict_sarsa_func, greedy_policy_sarsa_func = Sarsa_value_func_approximation(my_mdp,feature_state_action,0.001,0,1000000,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -0.6087862053619488,\n",
       " (0, 1): 0.7366269402309574,\n",
       " (0, 2): 4.17471518230005,\n",
       " (0, 3): 5.528982427311625,\n",
       " (1, 0): 0.5086764723772177,\n",
       " (1, 1): 1.7353188274122966,\n",
       " (1, 3): 6.99832751323632,\n",
       " (2, 0): 1.6156745265934085,\n",
       " (2, 1): 3.209931648439985,\n",
       " (2, 2): 5.0487797712236215,\n",
       " (2, 3): 8.367138593020785,\n",
       " (3, 0): 0.3588144875045164,\n",
       " (3, 1): 0.600767770421827,\n",
       " (3, 2): -10,\n",
       " (3, 3): 10}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value_dict_sarsa_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bench mark MDP optimal value function is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 1.71009019838139,\n",
       " (0, 1): 3.071932119775163,\n",
       " (0, 2): 4.449914383561644,\n",
       " (0, 3): 5.699914383561644,\n",
       " (1, 0): 0.8153548272312068,\n",
       " (1, 1): 2.0480740094833125,\n",
       " (1, 3): 7.106164383561644,\n",
       " (2, 0): 1.9112649294496935,\n",
       " (2, 1): 3.4521639072648256,\n",
       " (2, 2): 5.205479452054796,\n",
       " (2, 3): 8.356164383561644,\n",
       " (3, 0): 0.679983209147123,\n",
       " (3, 1): 0.8297294467265717,\n",
       " (3, 2): -10.0,\n",
       " (3, 3): 10.0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp_value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is less accurate than the non-parametric sarsa(0) algorithm even we iterate for a longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (0, 1): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (0, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (1, 1): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (2, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (3, 0): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 1): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (3, 3): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_policy_sarsa_func_final = {}\n",
    "for state in my_mdp.states:\n",
    "    greedy = {}\n",
    "    for action in my_mdp.actions:\n",
    "        greedy[action] = round(greedy_policy_sarsa_func[state][action])\n",
    "    greedy_policy_sarsa_func_final[state] = greedy\n",
    "greedy_policy_sarsa_func_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark MDP optimal policy is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 0): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 1): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (2, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (3, 0): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 1): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (3, 3): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy is less accurate as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Q-Learning control "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_func_approximation(mdp,feature_rep,alpha,lambda_TD,n,epsilon):\n",
    "    count = 0    \n",
    "    state_action_value_dict = {}\n",
    "    epsilon_greedy_policy = {}\n",
    "    m = len(mdp.states)\n",
    "    l = len(mdp.actions)\n",
    "    w = np.random.randn(m*l)\n",
    "      \n",
    "    for state in mdp.states:\n",
    "        action_value_dict = {}\n",
    "        for action in mdp.actions:\n",
    "            if state == (3,3):\n",
    "                action_value_dict[action] = 10\n",
    "            elif state == (3,2):\n",
    "                action_value_dict[action] = -10\n",
    "            else:\n",
    "                action_value_dict[action] = np.dot(feature_rep[state][action],w)\n",
    "        epsilon_greedy_policy[state] = epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon)\n",
    "        state_action_value_dict[state] = action_value_dict\n",
    "        \n",
    "    while count < n:\n",
    "        count += 1\n",
    "        s,a = mdp.generate_initial_state_action(epsilon_greedy_policy)\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            count += 1\n",
    "            s_next, r = mdp.simulate_transition(s,a)\n",
    "            max_action_value = -10000\n",
    "            for action in mdp.actions:\n",
    "                if state_action_value_dict[s_next][action]>max_action_value: max_action_value = state_action_value_dict[s_next][action]\n",
    "            # update Q(S,A) following the greedy policy pi\n",
    "            w += alpha*(r+mdp.gamma*max_action_value - np.dot(feature_rep[s][a],w))*feature_rep[s][a]\n",
    "            # update Q(s,a) and apply the epsilon-greedy policy improvement\n",
    "            for state in mdp.states:\n",
    "                action_value_dict = {}\n",
    "                for action in mdp.actions:\n",
    "                    if state == (3,3):\n",
    "                        action_value_dict[action] = 10\n",
    "                    elif state == (3,2):\n",
    "                        action_value_dict[action] = -10\n",
    "                    else:\n",
    "                        action_value_dict[action] = np.dot(feature_rep[state][action],w)\n",
    "                epsilon_greedy_policy[state] = epsilon_greedy_policy_improvement(mdp,action_value_dict,l,epsilon)\n",
    "                state_action_value_dict[state] = action_value_dict\n",
    "            s = s_next\n",
    "            # sample A using the epsilon-greedy policy\n",
    "            u1 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action in mdp.actions:\n",
    "                cum_proba += epsilon_greedy_policy[s][action]\n",
    "                if u1 <= cum_proba:\n",
    "                    a = action\n",
    "                    break\n",
    "\n",
    "    # finally we extract the optimal action-value function for each state\n",
    "    state_value_dict = {}\n",
    "    for state in mdp.states:\n",
    "        max_action_value = -10000\n",
    "        for action in mdp.actions:\n",
    "            if state_action_value_dict[state][action]>max_action_value:\n",
    "                max_action_value = state_action_value_dict[state][action]\n",
    "        state_value_dict[state] = max_action_value\n",
    "    return state_value_dict      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -2.3216445075736973,\n",
       " (0, 1): -1.756378166046778,\n",
       " (0, 2): -0.10396899756069912,\n",
       " (0, 3): 2.2819146078754713,\n",
       " (1, 0): -2.5451031119913825,\n",
       " (1, 1): -2.3505910595643833,\n",
       " (1, 3): 5.429967900250688,\n",
       " (2, 0): -2.5258899864261273,\n",
       " (2, 1): -0.885652515799276,\n",
       " (2, 2): 2.960619580835856,\n",
       " (2, 3): 7.8850191263841936,\n",
       " (3, 0): -2.883300550315505,\n",
       " (3, 1): -2.966706117288369,\n",
       " (3, 2): -10,\n",
       " (3, 3): 10}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_learning_func_approximation(my_mdp,feature_state_action,0.001,0,100000,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning algorithm diverges with value approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Policy Gradient - REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Monte_Carlo_policy_gradient(mdp,feature_action_rep,k,alpha):\n",
    "    # random initialization of weights\n",
    "    theta = np.random.uniform(0,1,len(mdp.actions)*len(mdp.states))\n",
    "    policy_dict = {}\n",
    "    for state in mdp.states:\n",
    "        policy_dict_state = {}\n",
    "        normalization = 0\n",
    "        for action in mdp.actions:\n",
    "            proba_raw = np.dot(feature_action_rep[state][action],theta)\n",
    "            policy_dict_state[action] = proba_raw\n",
    "            normalization += proba_raw\n",
    "        for action in mdp.actions:\n",
    "            policy_dict_state[action] /= normalization\n",
    "        policy_dict[state] = policy_dict_state\n",
    "    \n",
    "    # simulate k episodes    \n",
    "    for j in range(k):\n",
    "        # simulate the initial state\n",
    "        s,a = mdp.generate_initial_state_action(policy_dict)\n",
    "        episode= [(0,s,a)]  \n",
    "        \n",
    "        # simulate the following transitions until hitting the terminal state\n",
    "        while not mdp.is_terminal(s,a):    # while the state is not self absorbing, i.e. is not the terminal state\n",
    "            s, r = mdp.simulate_transition(s,a)\n",
    "            u2 = np.random.uniform(0,1)\n",
    "            cum_proba = 0\n",
    "            for action,proba in policy_dict[s].items():\n",
    "                cum_proba += proba\n",
    "                if u2 <= cum_proba:\n",
    "                    a = action\n",
    "                    break \n",
    "            episode.append((r,s,a))\n",
    "        \n",
    "        # prepare the \"training data\" (S_t, G_t)\n",
    "        T = len(episode)\n",
    "        G_vec = np.zeros(T)\n",
    "        for t in range(T):\n",
    "            G = 0\n",
    "            if t < T-1:\n",
    "                for j in range(t+1,T): G += episode[j][0]*mdp.gamma**(j-1-t)\n",
    "            end_reward = 10 if episode[T-1][1] == (3,3) else -10\n",
    "            G += end_reward*mdp.gamma**(T-1-t) \n",
    "            G_vec[t] = G\n",
    "        \n",
    "        # stochastic gradient descent     \n",
    "        perm = np.random.permutation(T)\n",
    "        for j in perm:\n",
    "            st = episode[j][1]\n",
    "            at = episode[j][2]\n",
    "            theta += alpha*G_vec[j]*feature_action_rep[st][at]/np.dot(feature_action_rep[st][at],theta)\n",
    "            for state in mdp.states:\n",
    "                policy_dict_state = {}\n",
    "                normalization = 0\n",
    "                for action in mdp.actions:\n",
    "                    proba_raw = np.dot(feature_action_rep[state][action],theta)\n",
    "                    policy_dict_state[action] = proba_raw\n",
    "                    normalization += proba_raw\n",
    "                for action in mdp.actions:\n",
    "                    policy_dict_state[action] /= normalization\n",
    "                policy_dict[state] = policy_dict_state\n",
    "    \n",
    "    \n",
    "    return policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 1.0002954143121976,\n",
       "  'Right': 0.2700764388135012,\n",
       "  'Down': -0.8441483823324725,\n",
       "  'Left': 0.5737765292067736},\n",
       " (0, 1): {'Up': 0.7027802621965207,\n",
       "  'Right': 0.24176202983028575,\n",
       "  'Down': 0.0715890455648956,\n",
       "  'Left': -0.016131337591701937},\n",
       " (0, 2): {'Up': 1.2122995498136961,\n",
       "  'Right': -0.2129654324977476,\n",
       "  'Down': -0.021286676266119103,\n",
       "  'Left': 0.02195255895017046},\n",
       " (0, 3): {'Up': -0.9029063736885005,\n",
       "  'Right': 2.979118665104538,\n",
       "  'Down': 0.003918619685761708,\n",
       "  'Left': -1.0801309111017992},\n",
       " (1, 0): {'Up': -0.03980072681119881,\n",
       "  'Right': 1.039545085559207,\n",
       "  'Down': 3.617435078809529,\n",
       "  'Left': -3.6171794375575375},\n",
       " (1, 1): {'Up': 1.0000488662111877,\n",
       "  'Right': 0.009736053378603945,\n",
       "  'Down': 0.08783281383056121,\n",
       "  'Left': -0.09761773342035297},\n",
       " (1, 3): {'Up': -0.0016685400234855464,\n",
       "  'Right': 1.0101497919848068,\n",
       "  'Down': 0.005862622717644371,\n",
       "  'Left': -0.014343874678965701},\n",
       " (2, 0): {'Up': 1.006530605245872,\n",
       "  'Right': 0.03250839832053037,\n",
       "  'Down': -0.03111195989134155,\n",
       "  'Left': -0.007927043675060858},\n",
       " (2, 1): {'Up': 1.0077061104701759,\n",
       "  'Right': -0.01155373318416458,\n",
       "  'Down': -0.0014001324654824546,\n",
       "  'Left': 0.005247755179471165},\n",
       " (2, 2): {'Up': 1.0020629916049386,\n",
       "  'Right': -0.006661997403285137,\n",
       "  'Down': -0.0031601293600327976,\n",
       "  'Left': 0.007759135158379378},\n",
       " (2, 3): {'Up': -0.0007884604730788403,\n",
       "  'Right': 1.0009309802785686,\n",
       "  'Down': -0.00827975652224731,\n",
       "  'Left': 0.00813723671675739},\n",
       " (3, 0): {'Up': -0.0011389618202675414,\n",
       "  'Right': 1.0019394380394642,\n",
       "  'Down': -1.3653978595352286e-05,\n",
       "  'Left': -0.0007868222406012596},\n",
       " (3, 1): {'Up': 0.9999967736162941,\n",
       "  'Right': -0.0011668075877846842,\n",
       "  'Down': -0.0034478666846084172,\n",
       "  'Left': 0.004617900656098928},\n",
       " (3, 2): {'Up': 1.0009085653434866,\n",
       "  'Right': -0.007043652791238625,\n",
       "  'Down': 0.01406590121700818,\n",
       "  'Left': -0.007930813769256197},\n",
       " (3, 3): {'Up': 0.24327405861176243,\n",
       "  'Right': 0.2534914609348157,\n",
       "  'Down': 0.2523185366392293,\n",
       "  'Left': 0.25091594381419263}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Monte_Carlo_policy_gradient(my_mdp,feature_state_action,1000000,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the algorithm diverges even after 1,000,000 iterations since some of the (normalized) probabilities are negative. One potential reason is that the action space is too small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
