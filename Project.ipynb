{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MP Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess(object):\n",
    "    \n",
    "    def __init__(self,transition_proba):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Process.\n",
    "        \n",
    "        Three attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \n",
    "        stationary_dist: dictionary of float\n",
    "            {state: proba} to represent the long-run probability of being in each state\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.stationary_dist = None\n",
    "    \n",
    "    def calculate_stationary_distribution(self):\n",
    "        # probability dictionary to matrix\n",
    "        num = len(self.states)\n",
    "        T = np.empty([num,num])\n",
    "        i = 0\n",
    "        for state, proba in self.transition_proba.items():\n",
    "            T[i,:] = list(proba.values())\n",
    "            i = i+1\n",
    "        \"\"\"\n",
    "        Stationary distribution is given by solving\n",
    "        pi = pi*T.\n",
    "        Solve for the left (row) eigen vector of transition matrix\n",
    "        \"\"\"      \n",
    "        value, left, right = scipy.linalg.eig(T, left=True,right=True)\n",
    "        eig_vec = left[:,np.where(np.abs(value-1)<1e-8)[0][0]].astype(float)\n",
    "        eig_vec = eig_vec/np.sum(eig_vec) # normalize\n",
    "        self.stationary_dist = {}\n",
    "        for i in range(num):\n",
    "            self.stationary_dist[self.states[i]] = eig_vec[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_0 = {'Facebook':{'Facebook':0.9,'Class 1':0.1,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 1':{'Facebook':0.5,'Class 1':0,'Class 2':0.5,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 2':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0.8,'Pass':0,'Sleep':0.2,'Pub':0},\n",
    "     'Class 3':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0.6,'Sleep':0,'Pub':0.4},\n",
    "     'Pass':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':1,'Pub':0},\n",
    "     'Sleep':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':1,'Pub':0},\n",
    "     'Pub':{'Facebook':0,'Class 1':0.2,'Class 2':0.4,'Class 3':0.4,'Pass':0,'Sleep':0,'Pub':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state space of the student MP problem:\n",
      "['Facebook', 'Class 1', 'Class 2', 'Class 3', 'Pass', 'Sleep', 'Pub']\n",
      "\n",
      "\n",
      "The stationary distribution of the student MP problem:\n",
      "{'Facebook': 0.0, 'Class 1': 0.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Pass': 0.0, 'Sleep': 1.0, 'Pub': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "mp = MarkovProcess(T_0)\n",
    "print('The state space of the student MP problem:')\n",
    "print(mp.states)\n",
    "print('\\n')\n",
    "print('The stationary distribution of the student MP problem:')\n",
    "mp.calculate_stationary_distribution()\n",
    "print(mp.stationary_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MRP Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess(object):\n",
    "    \n",
    "    def __init__(self,transition_proba,reward,gamma):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovRewardProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            (Reward) Process.\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "            {state : {state : reward}} representing the immediate reward\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Reward Process.\n",
    "        \n",
    "        gamma: float\n",
    "            a float between 0 and 1 to discount long-run rewards\n",
    "            \n",
    "        Five attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "        \n",
    "        gamma: float\n",
    "        \n",
    "        values: dictionary of float\n",
    "            {state: value} to represent the optimal value function of each state\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.values = None\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"\n",
    "        Check whether each state is self-absorbing\n",
    "        \n",
    "        return a dictionary of bools\n",
    "        \"\"\"\n",
    "        is_terminal_dict = {s: t[s] == 1 for s,t in self.transition_proba.items()}\n",
    "        return is_terminal_dict\n",
    "            \n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        A helper function to transform the transition dictionary to a 2-D matrix\n",
    "        \"\"\"\n",
    "        num = len(self.states)\n",
    "        T = np.empty([num,num])\n",
    "        for i in range(num):\n",
    "            for j in range(num):\n",
    "                T[i,j] = self.transition_proba[self.states[i]][self.states[j]]\n",
    "        return T\n",
    "    \n",
    "    def get_expected_reward(self):\n",
    "        \"\"\"\n",
    "        Transform the r(s,s') representation to R(s) = E[r(s,s')] representation\n",
    "        Return a dictionary {state: reward}\n",
    "        \"\"\"\n",
    "        T = self.get_transition_matrix()\n",
    "        num = len(self.states)\n",
    "        R = np.empty([num,num])\n",
    "        for i in range(num):\n",
    "            for j in range(num):\n",
    "                R[i,j] = self.reward[self.states[i]][self.states[j]]        \n",
    "        R_vec = np.diagonal(np.dot(T,R.T))\n",
    "        R_dict = {}\n",
    "        for i in range(num):\n",
    "            R_dict[self.states[i]] = R_vec[i]\n",
    "        return R_dict\n",
    "    \n",
    "    def get_value_function(self):\n",
    "        \"\"\"\n",
    "        Solve Bellman Equations \n",
    "            u = R + T*u\n",
    "        s.t. the terminal conditions\n",
    "        \n",
    "        Return a dictionary {state:value}\n",
    "        \"\"\"\n",
    "        R_dict = self.get_expected_reward()\n",
    "        is_terminal_dict = self.is_terminal()\n",
    "        R = np.empty(len(self.states))\n",
    "        is_terminal = list()\n",
    "        for i in range(len(self.states)):\n",
    "            R[i] = R_dict[self.states[i]]\n",
    "            is_terminal.append(is_terminal_dict[self.states[i]])\n",
    "        is_terminal = np.array(is_terminal)\n",
    "        T = self.get_transition_matrix()\n",
    "        v_vec = np.empty(len(self.states))\n",
    "        # terminal states\n",
    "        v_vec[is_terminal] = R[is_terminal]\n",
    "        # solve bellman equations for non-terminal states\n",
    "        v_recur = v_vec[~is_terminal]\n",
    "        R_inter = R[~is_terminal]+ np.dot(T[:,is_terminal][~is_terminal],v_vec[is_terminal])\n",
    "        v_recur = np.linalg.solve(self.gamma*T[:,~is_terminal][~is_terminal]-np.eye(T[:,~is_terminal][~is_terminal].shape[0]),-R_inter)\n",
    "        v_vec[~is_terminal] = v_recur\n",
    "        self.values={}\n",
    "        for i in range(len(self.states)):\n",
    "            self.values[self.states[i]] = v_vec[i]\n",
    "        return self.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_0 = {'Facebook':{'Facebook':-1,'Class 1':-1,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 1':{'Facebook':-2,'Class 1':0,'Class 2':-2,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 2':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':-2,'Pass':0,'Sleep':-2,'Pub':0},\n",
    "     'Class 3':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':-2,'Sleep':0,'Pub':-2},\n",
    "     'Pass':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':10,'Pub':0},\n",
    "     'Sleep':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Pub':{'Facebook':0,'Class 1':1,'Class 2':1,'Class 3':1,'Pass':0,'Sleep':0,'Pub':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expectation of immediate reward of each state is\n",
      "{'Facebook': -1.0, 'Class 1': -2.0, 'Class 2': -2.0, 'Class 3': -2.0, 'Pass': 10.0, 'Sleep': 0.0, 'Pub': 1.0}\n",
      "\n",
      "\n",
      "The long-run value of each state is\n",
      "{'Facebook': -7.637608431059512, 'Class 1': -5.0127289100145225, 'Class 2': 0.9426552976939073, 'Class 3': 4.087021246797093, 'Pass': 10.0, 'Sleep': -2.026954639512266e-16, 'Pub': 1.908392352214145}\n"
     ]
    }
   ],
   "source": [
    "my_mrp = MarkovRewardProcess(T_0,R_0,0.9)\n",
    "print('The expectation of immediate reward of each state is')\n",
    "print(my_mrp.get_expected_reward())\n",
    "print('\\n')\n",
    "print('The long-run value of each state is')\n",
    "print(my_mrp.get_value_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MDP Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess(object):\n",
    "        def __init__(self,transition_proba,reward,gamma,tol):\n",
    "            \"\"\"\n",
    "            Initialize the MarkovDecisionProcess instance.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            transition_proba: dictionary of dictionary of dictionary\n",
    "                {state : {action: {state : probability}}} representing the probabilities\n",
    "                of transitioning from one state to another in the Markov \n",
    "                Decision Process.\n",
    "\n",
    "            reward: dictionary of dictionary of dictionary\n",
    "                {state : {action: {state : reward}}} representing the immediate reward\n",
    "                of transitioning from one state to another in the Markov \n",
    "                Decision Process.\n",
    "\n",
    "            gamma: float\n",
    "                a float between 0 and 1 to discount long-run rewards\n",
    "            \n",
    "            tol: float\n",
    "                the tolerance level of error in iteration\n",
    "                \n",
    "            Eight attributes\n",
    "            ----------\n",
    "            transition_proba\n",
    "\n",
    "            states: vector\n",
    "                a vector to represent the state space of the Markov Process\n",
    "    \n",
    "            actions: vector\n",
    "                a vector to represent the action space of the Markov\n",
    "            \n",
    "            reward\n",
    "\n",
    "            gamma\n",
    "            \n",
    "            tol\n",
    "            \n",
    "            policy: dictionary of dictionary\n",
    "                {state: {action: proba}} to represent the stochastic policy\n",
    "            \n",
    "            values: dictionary of float\n",
    "                {state: value} to represent the optimal value function of each state\n",
    "            \"\"\"\n",
    "            self.transition_proba = transition_proba\n",
    "            self.reward = reward\n",
    "            self.gamma = gamma\n",
    "            self.tol = tol\n",
    "            self.states = list(transition_proba.keys())\n",
    "            self.actions = list(transition_proba[self.states[0]].keys())\n",
    "\n",
    "            self.policy = None\n",
    "            self.values = None\n",
    "\n",
    "        \n",
    "        def dict_to_matrix_1(self,dictionary):        \n",
    "            M = np.empty([len(self.states),len(self.actions),len(self.states)])\n",
    "            for i in range(len(self.states)):\n",
    "                for a in range(len(self.actions)):\n",
    "                    for j in range(len(self.states)):\n",
    "                        M[i][a][j]= dictionary[self.states[i]][self.actions[a]][self.states[j]]\n",
    "            return M\n",
    "        \n",
    "        def dict_to_matrix_2(self,dictionary):\n",
    "            M = np.empty([len(self.states),len(self.actions)])\n",
    "            for i in range(len(self.states)):\n",
    "                for a in range(len(self.actions)):\n",
    "                    M[i][a]= dictionary[self.states[i]][self.actions[a]]\n",
    "            return M\n",
    "        \n",
    "        def get_expected_reward(self):\n",
    "            \"\"\"\n",
    "            Calculate the expected reward per (state,action)\n",
    "            \n",
    "            Return {state: {action: reward}}\n",
    "            \"\"\"  \n",
    "            R = self.dict_to_matrix_1(self.reward)\n",
    "            T = self.dict_to_matrix_1(self.transition_proba)\n",
    "            R_expected_dict = {}\n",
    "            for i in range(len(self.states)):\n",
    "                R_state_reward_vec = np.diagonal(np.dot(T[i][:][:],(R[i][:][:]).T))\n",
    "                R_state_reward = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    R_state_reward[self.actions[a]] = R_state_reward_vec[a]\n",
    "                R_expected_dict[self.states[i]]= R_state_reward\n",
    "            return R_expected_dict\n",
    "        \n",
    "        def to_MRP(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, represent the problem as a Markov Reward Process\n",
    "            \"\"\"       \n",
    "            num = len(self.states)\n",
    "            \n",
    "            T_dict = {}\n",
    "            T_matrix = self.dict_to_matrix_1(self.transition_proba)\n",
    "            for i in range(num):\n",
    "                T_state_vec = np.zeros(num)\n",
    "                T_state_dict = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    T_state_vec += policy_dict[self.states[i]][self.actions[a]]*T_matrix[i][a][:]\n",
    "                for j in range(num):\n",
    "                    T_state_dict[self.states[j]] = T_state_vec[j]\n",
    "                T_dict[self.states[i]] = T_state_dict\n",
    "            \n",
    "            R_dict = {}\n",
    "            R_matrix = self.dict_to_matrix_1(self.reward)\n",
    "            for i in range(num):\n",
    "                R_state_vec = np.zeros(num)\n",
    "                R_state_dict = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    R_state_vec += policy_dict[self.states[i]][self.actions[a]]*R_matrix[i][a][:]\n",
    "                for j in range(num):\n",
    "                    R_state_dict[self.states[j]] = R_state_vec[j]\n",
    "                R_dict[self.states[i]] = R_state_dict            \n",
    "            \n",
    "            return MarkovRewardProcess(T_dict,R_dict,self.gamma)\n",
    "        \n",
    "        def get_state_value_function(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, return the value function of the corresponding MRP problem\n",
    "            \n",
    "            Return {state: value}\n",
    "            \"\"\"                 \n",
    "            mrp = self.to_MRP(policy_dict)\n",
    "            values = mrp.get_value_function()\n",
    "            return values\n",
    "        \n",
    "        def get_state_act_value_function(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, return the action value function of the corresponding MRP problem\n",
    "            \n",
    "            Return {state: {action: value}}\n",
    "            \"\"\" \n",
    "            v_dict = self.get_state_value_function(policy_dict)\n",
    "            v_vec = np.empty(len(self.states))\n",
    "            for i in range(len(self.states)):\n",
    "                v_vec[i] = v_dict[self.states[i]]\n",
    "                \n",
    "            R_expected_dict = self.get_expected_reward()\n",
    "            R_matrix = self.dict_to_matrix_2(R_expected_dict)\n",
    "                    \n",
    "            T = self.dict_to_matrix_1(self.transition_proba)\n",
    "            \n",
    "            q = {}\n",
    "            q_matrix = R_matrix + np.dot(T,v_vec)\n",
    "            for i in range(len(self.states)):\n",
    "                q_state = {}\n",
    "                for a in range(len(self.actions)):\n",
    "                    q_state[self.actions[a]] = q_matrix[i][a]\n",
    "                q[self.states[i]] = q_state\n",
    "    \n",
    "            return q\n",
    "        \n",
    "        def get_improved_policy(self,policy_dict):\n",
    "            \"\"\"\n",
    "            Given a policy, find the best possible action for each state based\n",
    "            on the action value function\n",
    "            \"\"\" \n",
    "            q = self.get_state_act_value_function(policy_dict)\n",
    "            greedy_policy = {}\n",
    "            for state,action_value in q.items():\n",
    "                policy_state = {}\n",
    "                greedy_action = max(action_value.items(),key=operator.itemgetter(1))[0]\n",
    "                for action in self.actions:\n",
    "                    if action == greedy_action: policy_state[action] = 1\n",
    "                    else: policy_state[action] = 0\n",
    "                greedy_policy[state] = policy_state\n",
    "            return greedy_policy\n",
    "        \n",
    "        def policy_iteration(self):\n",
    "            # initialization\n",
    "            policy_dict = {s:{a:1./len(self.actions) for a in self.actions } for s in self.states }\n",
    "            v_max = self.get_state_value_function(policy_dict)\n",
    "            epsilon = 1000\n",
    "            while epsilon >= self.tol:\n",
    "                policy_dict = self.get_improved_policy(policy_dict)\n",
    "                v = self.get_state_value_function(policy_dict)\n",
    "                epsilon = max(abs(v[s] - v_max[s]) for s in v) \n",
    "                v_max = v\n",
    "            \n",
    "            return policy_dict\n",
    "        \n",
    "        def value_iteration(self):\n",
    "            v = np.zeros(len(self.states))\n",
    "            T = self.dict_to_matrix_1(self.transition_proba)\n",
    "            R_matrix = self.dict_to_matrix_2(self.get_expected_reward())\n",
    "\n",
    "            iterate = True           \n",
    "            while iterate:\n",
    "                OldValues = v.copy()\n",
    "                v = np.max(R_matrix+self.gamma*np.dot(T,OldValues),axis=1)\n",
    "                if np.max(np.abs(v-OldValues)) < self.tol:\n",
    "                    iterate = False\n",
    "            \n",
    "            self.values={}\n",
    "            for i in range(num_s):\n",
    "                self.values[self.states[i]] = v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: smoov & curly's bogus journey\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLw0qyFP7t4IYuI9XcDIGsARfO1QKMrpbN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = {}\n",
    "R = {}\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        if not(i==1 and j == 2):\n",
    "            T_state = {}\n",
    "            R_state = {}\n",
    "            for direction in ['Up','Right','Down','Left']:\n",
    "                T_state_action = {}\n",
    "                R_state_action = {}\n",
    "                for m in range(4):\n",
    "                    for n in range(4):\n",
    "                        if not(m==1 and n == 2):\n",
    "                            T_state_action[(m,n)] = 0\n",
    "                            if i==3 and j == 3: R_state_action[(m,n)] = 10\n",
    "                            elif i == 3 and j == 2: R_state_action[(m,n)] = -10\n",
    "                            else: R_state_action[(m,n)] = -1\n",
    "                T_state[direction] = T_state_action\n",
    "                R_state[direction] = R_state_action\n",
    "            T[(i,j)] = T_state\n",
    "            R[(i,j)] = R_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "T[(0,0)]['Up'][(0,1)] = 0.8\n",
    "T[(0,0)]['Up'][(1,0)] = 0.1\n",
    "T[(0,0)]['Up'][(0,0)] = 0.1\n",
    "T[(0,0)]['Right'][(0,1)] = 0.1\n",
    "T[(0,0)]['Right'][(1,0)] = 0.8\n",
    "T[(0,0)]['Right'][(0,0)] = 0.1\n",
    "T[(0,0)]['Left'][(0,0)] = 0.9\n",
    "T[(0,0)]['Left'][(0,1)] = 0.1\n",
    "T[(0,0)]['Down'][(0,1)] = 0.1\n",
    "T[(0,0)]['Down'][(0,0)] = 0.9\n",
    "\n",
    "T[(0,1)]['Up'][(0,2)] = 0.8\n",
    "T[(0,1)]['Up'][(0,1)] = 0.1\n",
    "T[(0,1)]['Up'][(1,1)] = 0.1\n",
    "T[(0,1)]['Right'][(1,1)] = 0.8\n",
    "T[(0,1)]['Right'][(0,2)] = 0.1\n",
    "T[(0,1)]['Right'][(0,0)] = 0.1\n",
    "T[(0,1)]['Left'][(0,1)] = 0.8\n",
    "T[(0,1)]['Left'][(0,0)] = 0.1\n",
    "T[(0,1)]['Left'][(0,2)] = 0.1\n",
    "T[(0,1)]['Down'][(0,0)] = 0.8\n",
    "T[(0,1)]['Down'][(0,1)] = 0.1\n",
    "T[(0,1)]['Down'][(1,1)] = 0.1\n",
    "\n",
    "T[(0,2)]['Up'][(0,3)] = 0.8\n",
    "T[(0,2)]['Up'][(0,2)] = 0.2\n",
    "T[(0,2)]['Right'][(0,2)] = 0.8\n",
    "T[(0,2)]['Right'][(0,3)] = 0.1\n",
    "T[(0,2)]['Right'][(0,1)] = 0.1\n",
    "T[(0,2)]['Left'][(0,2)] = 0.8\n",
    "T[(0,2)]['Left'][(0,3)] = 0.1\n",
    "T[(0,2)]['Left'][(0,1)] = 0.1\n",
    "T[(0,2)]['Down'][(0,1)] = 0.8\n",
    "T[(0,2)]['Down'][(0,2)] = 0.2\n",
    "\n",
    "T[(0,3)]['Up'][(0,3)] = 0.9\n",
    "T[(0,3)]['Up'][(1,3)] = 0.1\n",
    "T[(0,3)]['Right'][(1,3)] = 0.8\n",
    "T[(0,3)]['Right'][(0,3)] = 0.1\n",
    "T[(0,3)]['Right'][(0,2)] = 0.1\n",
    "T[(0,3)]['Left'][(0,3)] = 0.9\n",
    "T[(0,3)]['Left'][(0,2)] = 0.1\n",
    "T[(0,3)]['Down'][(0,2)] = 0.8\n",
    "T[(0,3)]['Down'][(0,3)] = 0.1\n",
    "T[(0,3)]['Down'][(1,3)] = 0.1\n",
    "\n",
    "T[(1,0)]['Up'][(1,1)] = 0.8\n",
    "T[(1,0)]['Up'][(0,0)] = 0.1\n",
    "T[(1,0)]['Up'][(2,0)] = 0.1\n",
    "T[(1,0)]['Right'][(2,0)] = 0.8\n",
    "T[(1,0)]['Right'][(1,0)] = 0.1\n",
    "T[(1,0)]['Right'][(1,1)] = 0.1\n",
    "T[(1,0)]['Left'][(0,0)] = 0.8\n",
    "T[(1,0)]['Left'][(1,0)] = 0.1\n",
    "T[(1,0)]['Left'][(1,1)] = 0.1\n",
    "T[(1,0)]['Down'][(1,0)] = 0.8\n",
    "T[(1,0)]['Down'][(2,0)] = 0.1\n",
    "T[(1,0)]['Down'][(0,0)] = 0.1\n",
    "\n",
    "T[(1,1)]['Up'][(1,1)] = 0.8\n",
    "T[(1,1)]['Up'][(0,1)] = 0.1\n",
    "T[(1,1)]['Up'][(2,1)] = 0.1\n",
    "T[(1,1)]['Right'][(2,1)] = 0.8\n",
    "T[(1,1)]['Right'][(1,1)] = 0.1\n",
    "T[(1,1)]['Right'][(1,0)] = 0.1\n",
    "T[(1,1)]['Left'][(0,1)] = 0.8\n",
    "T[(1,1)]['Left'][(1,0)] = 0.1\n",
    "T[(1,1)]['Left'][(1,1)] = 0.1\n",
    "T[(1,1)]['Down'][(1,0)] = 0.8\n",
    "T[(1,1)]['Down'][(2,1)] = 0.1\n",
    "T[(1,1)]['Down'][(0,1)] = 0.1\n",
    "\n",
    "T[(1,3)]['Up'][(1,3)] = 0.8\n",
    "T[(1,3)]['Up'][(0,3)] = 0.1\n",
    "T[(1,3)]['Up'][(2,3)] = 0.1\n",
    "T[(1,3)]['Right'][(2,3)] = 0.8\n",
    "T[(1,3)]['Right'][(1,3)] = 0.2\n",
    "T[(1,3)]['Left'][(0,3)] = 0.8\n",
    "T[(1,3)]['Left'][(1,3)] = 0.2\n",
    "T[(1,3)]['Down'][(1,3)] = 0.8\n",
    "T[(1,3)]['Down'][(0,3)] = 0.1\n",
    "T[(1,3)]['Down'][(2,3)] = 0.1\n",
    "\n",
    "T[(2,0)]['Up'][(2,1)] = 0.8\n",
    "T[(2,0)]['Up'][(1,0)] = 0.1\n",
    "T[(2,0)]['Up'][(3,0)] = 0.1\n",
    "T[(2,0)]['Right'][(3,0)] = 0.8\n",
    "T[(2,0)]['Right'][(2,0)] = 0.1\n",
    "T[(2,0)]['Right'][(2,1)] = 0.1\n",
    "T[(2,0)]['Left'][(1,0)] = 0.8\n",
    "T[(2,0)]['Left'][(2,0)] = 0.1\n",
    "T[(2,0)]['Left'][(2,1)] = 0.1\n",
    "T[(2,0)]['Down'][(1,0)] = 0.1\n",
    "T[(2,0)]['Down'][(2,0)] = 0.8\n",
    "T[(2,0)]['Down'][(3,0)] = 0.1\n",
    "\n",
    "T[(2,1)]['Up'][(2,2)] = 0.8\n",
    "T[(2,1)]['Up'][(1,1)] = 0.1\n",
    "T[(2,1)]['Up'][(3,1)] = 0.1\n",
    "T[(2,1)]['Right'][(3,1)] = 0.8\n",
    "T[(2,1)]['Right'][(2,0)] = 0.1\n",
    "T[(2,1)]['Right'][(2,2)] = 0.1\n",
    "T[(2,1)]['Left'][(1,1)] = 0.8\n",
    "T[(2,1)]['Left'][(2,0)] = 0.1\n",
    "T[(2,1)]['Left'][(2,2)] = 0.1\n",
    "T[(2,1)]['Down'][(1,1)] = 0.1\n",
    "T[(2,1)]['Down'][(2,0)] = 0.8\n",
    "T[(2,1)]['Down'][(3,1)] = 0.1\n",
    "\n",
    "T[(2,2)]['Up'][(2,3)] = 0.8\n",
    "T[(2,2)]['Up'][(2,2)] = 0.1\n",
    "T[(2,2)]['Up'][(3,2)] = 0.1\n",
    "T[(2,2)]['Right'][(3,1)] = 0.8\n",
    "T[(2,2)]['Right'][(2,0)] = 0.1\n",
    "T[(2,2)]['Right'][(2,2)] = 0.1\n",
    "T[(2,2)]['Left'][(1,1)] = 0.8\n",
    "T[(2,2)]['Left'][(2,0)] = 0.1\n",
    "T[(2,2)]['Left'][(2,2)] = 0.1\n",
    "T[(2,2)]['Down'][(2,2)] = 0.1\n",
    "T[(2,2)]['Down'][(2,1)] = 0.8\n",
    "T[(2,2)]['Down'][(3,2)] = 0.1\n",
    "\n",
    "T[(2,3)]['Up'][(2,3)] = 0.8\n",
    "T[(2,3)]['Up'][(1,3)] = 0.1\n",
    "T[(2,3)]['Up'][(3,3)] = 0.1\n",
    "T[(2,3)]['Right'][(3,3)] = 0.8\n",
    "T[(2,3)]['Right'][(2,3)] = 0.1\n",
    "T[(2,3)]['Right'][(2,2)] = 0.1\n",
    "T[(2,3)]['Left'][(1,3)] = 0.8\n",
    "T[(2,3)]['Left'][(2,2)] = 0.1\n",
    "T[(2,3)]['Left'][(2,3)] = 0.1\n",
    "T[(2,3)]['Down'][(1,3)] = 0.1\n",
    "T[(2,3)]['Down'][(2,2)] = 0.8\n",
    "T[(2,3)]['Down'][(3,3)] = 0.1\n",
    "\n",
    "T[(3,0)]['Up'][(3,1)] = 0.8\n",
    "T[(3,0)]['Up'][(2,0)] = 0.1\n",
    "T[(3,0)]['Up'][(3,0)] = 0.1\n",
    "T[(3,0)]['Right'][(3,0)] = 0.9\n",
    "T[(3,0)]['Right'][(3,1)] = 0.1\n",
    "T[(3,0)]['Left'][(2,0)] = 0.8\n",
    "T[(3,0)]['Left'][(3,0)] = 0.1\n",
    "T[(3,0)]['Left'][(3,1)] = 0.1\n",
    "T[(3,0)]['Down'][(3,0)] = 0.9\n",
    "T[(3,0)]['Down'][(2,0)] = 0.1\n",
    "\n",
    "T[(3,1)]['Up'][(3,2)] = 0.8\n",
    "T[(3,1)]['Up'][(2,1)] = 0.1\n",
    "T[(3,1)]['Up'][(3,1)] = 0.1\n",
    "T[(3,1)]['Right'][(3,1)] = 0.8\n",
    "T[(3,1)]['Right'][(3,0)] = 0.1\n",
    "T[(3,1)]['Right'][(3,2)] = 0.1\n",
    "T[(3,1)]['Left'][(2,1)] = 0.8\n",
    "T[(3,1)]['Left'][(3,0)] = 0.1\n",
    "T[(3,1)]['Left'][(3,2)] = 0.1\n",
    "T[(3,1)]['Down'][(3,0)] = 0.8\n",
    "T[(3,1)]['Down'][(2,1)] = 0.1\n",
    "T[(3,1)]['Down'][(3,1)] = 0.1\n",
    "\n",
    "for direction in ['Up','Right','Down','Left']:\n",
    "    T[(3,2)][direction][(3,2)] = 1\n",
    "    T[(3,3)][direction][(3,3)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mdp = MarkovDecisionProcess(T,R,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (0, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 0): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 1): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (1, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (2, 0): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 1): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (2, 3): {'Up': 0, 'Right': 1, 'Down': 0, 'Left': 0},\n",
       " (3, 0): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 1): {'Up': 0, 'Right': 0, 'Down': 0, 'Left': 1},\n",
       " (3, 2): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0},\n",
       " (3, 3): {'Up': 1, 'Right': 0, 'Down': 0, 'Left': 0}}"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy = my_mdp.policy_iteration()\n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mdp.value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mdp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
