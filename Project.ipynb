{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MP Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess(object):\n",
    "    \n",
    "    def __init__(self,transition_proba):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Process.\n",
    "        \n",
    "        Two attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.stationary_dist = None\n",
    "    \n",
    "    def calculate_stationary_distribution(self):\n",
    "        # probability dictionary to matrix\n",
    "        num = len(self.states)\n",
    "        T = np.empty([num,num])\n",
    "        i = 0\n",
    "        for state, proba in self.transition_proba.items():\n",
    "            T[i,:] = list(proba.values())\n",
    "            i = i+1\n",
    "        \"\"\"\n",
    "        Stationary distribution is given by solving\n",
    "        pi = pi*T.\n",
    "        Solve for the left (row) eigen vector of transition matrix\n",
    "        \"\"\"      \n",
    "        value, left, right = scipy.linalg.eig(T, left=True,right=True)\n",
    "        eig_vec = left[:,np.where(np.abs(value-1)<1e-8)[0][0]].astype(float)\n",
    "        eig_vec = eig_vec/np.sum(eig_vec) # normalize\n",
    "        self.stationary_dist = {}\n",
    "        for i in range(num):\n",
    "            self.stationary_dist[self.states[i]] = eig_vec[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = {'Facebook':{'Facebook':0.9,'Class 1':0.1,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 1':{'Facebook':0.5,'Class 1':0,'Class 2':0.5,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 2':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0.8,'Pass':0,'Sleep':0.2,'Pub':0},\n",
    "     'Class 3':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0.6,'Sleep':0,'Pub':0.4},\n",
    "     'Pass':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':1,'Pub':0},\n",
    "     'Sleep':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':1,'Pub':0},\n",
    "     'Pub':{'Facebook':0,'Class 1':0.2,'Class 2':0.4,'Class 3':0.4,'Pass':0,'Sleep':0,'Pub':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state space of the student MP problem:\n",
      "['Facebook', 'Class 1', 'Class 2', 'Class 3', 'Pass', 'Sleep', 'Pub']\n",
      "\n",
      "\n",
      "The stationary distribution of the student MP problem:\n",
      "{'Facebook': 0.0, 'Class 1': 0.0, 'Class 2': 0.0, 'Class 3': 0.0, 'Pass': 0.0, 'Sleep': 1.0, 'Pub': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    }
   ],
   "source": [
    "mp = MarkovProcess(T)\n",
    "print('The state space of the student MP problem:')\n",
    "print(mp.states)\n",
    "print('\\n')\n",
    "print('The stationary distribution of the student MP problem:')\n",
    "mp.calculate_stationary_distribution()\n",
    "print(mp.stationary_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MRP Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess(object):\n",
    "    \n",
    "    def __init__(self,transition_proba,reward,gamma):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovRewardProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            (Reward) Process.\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "            {state : {state : reward}} representing the immediate reward\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Reward Process.\n",
    "        \n",
    "        gamma: float\n",
    "            a float between 0 and 1 to discount long-run rewards\n",
    "            \n",
    "        Four attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "        \n",
    "        gamma: float\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.values = None\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        num = len(self.states)\n",
    "        T = np.empty([num,num])\n",
    "        i = 0\n",
    "        for state, proba in self.transition_proba.items():\n",
    "            T[i,:] = list(proba.values())\n",
    "            i = i+1\n",
    "        return T\n",
    "    \n",
    "    def get_expected_reward(self):\n",
    "        T = self.get_transition_matrix()\n",
    "        num = len(self.states)\n",
    "        R = np.empty([num,num])\n",
    "        i = 0\n",
    "        for state in self.states:\n",
    "            reward_dic = self.reward[state]\n",
    "            R[i,:] = list(reward_dic.values())\n",
    "            i = i+1  \n",
    "        R_vec = np.diagonal(np.dot(T,R.T))\n",
    "        R_dict = {}\n",
    "        for i in range(num):\n",
    "            R_dict[self.states[i]] = R_vec[i]\n",
    "        return R_dict\n",
    "    \n",
    "    def value_function(self):\n",
    "        R = np.array(list(self.get_expected_reward().values()))\n",
    "        T = self.get_transition_matrix()\n",
    "        v_vec = np.linalg.solve(self.gamma*T-np.eye(T.shape[0]),-R)\n",
    "        self.values={}\n",
    "        for i in range(len(self.states)):\n",
    "            self.values[self.states[i]] = v_vec[i]\n",
    "        return self.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {'Facebook':{'Facebook':-1,'Class 1':-1,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 1':{'Facebook':-2,'Class 1':0,'Class 2':-2,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Class 2':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':-2,'Pass':0,'Sleep':-2,'Pub':0},\n",
    "     'Class 3':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':-2,'Sleep':0,'Pub':-2},\n",
    "     'Pass':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':10,'Pub':0},\n",
    "     'Sleep':{'Facebook':0,'Class 1':0,'Class 2':0,'Class 3':0,'Pass':0,'Sleep':0,'Pub':0},\n",
    "     'Pub':{'Facebook':0,'Class 1':1,'Class 2':1,'Class 3':1,'Pass':0,'Sleep':0,'Pub':0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expectation of immediate reward of each state is\n",
      "{'Facebook': -1.0, 'Class 1': -2.0, 'Class 2': -2.0, 'Class 3': -2.0, 'Pass': 10.0, 'Sleep': 0.0, 'Pub': 1.0}\n",
      "\n",
      "\n",
      "The long-run value of each state is\n",
      "{'Facebook': -7.637608431059512, 'Class 1': -5.0127289100145225, 'Class 2': 0.9426552976939073, 'Class 3': 4.087021246797093, 'Pass': 10.0, 'Sleep': -2.026954639512266e-16, 'Pub': 1.908392352214145}\n"
     ]
    }
   ],
   "source": [
    "my_mrp = MarkovRewardProcess(T,R,0.9)\n",
    "print('The expectation of immediate reward of each state is')\n",
    "print(my_mrp.get_expected_reward())\n",
    "print('\\n')\n",
    "print('The long-run value of each state is')\n",
    "print(my_mrp.value_function())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MDP Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess(object):\n",
    "        def __init__(self,transition_proba,reward,gamma,tol):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovRewardProcess instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "            {state : {state : probability}} representing the probabilities\n",
    "            of transitioning from one state to another in the Markov \n",
    "            (Reward) Process.\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "            {state : {state : reward}} representing the immediate reward\n",
    "            of transitioning from one state to another in the Markov \n",
    "            Reward Process.\n",
    "        \n",
    "        gamma: float\n",
    "            a float between 0 and 1 to discount long-run rewards\n",
    "            \n",
    "        Four attributes\n",
    "        ----------\n",
    "        transition_proba: dictionary of dictionary\n",
    "        \n",
    "        states: vector\n",
    "            a vector to represent the state space of the Markov Process\n",
    "        \n",
    "        reward: dictionary of dictionary\n",
    "        \n",
    "        gamma: float\n",
    "        \"\"\"\n",
    "        self.transition_proba = transition_proba\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "        self.tol = tol\n",
    "        self.states = list(transition_proba.keys())\n",
    "        self.actions = list(transition_proba[self.states[0]].keys())\n",
    "        \n",
    "        self.policy = None\n",
    "        self.values = None\n",
    "\n",
    "        def to_MRP(self,policy_dict):\n",
    "            T = {}\n",
    "            R = {}\n",
    "            for state,policy in policy_dict.items():\n",
    "                T[state] = self.transition_proba[state][policy]\n",
    "                R[state] = self.reward[state][policy]   \n",
    "            return MarkovRewardProcess(T,R,self.gamma)\n",
    "        \n",
    "        def get_transition_matrix(self):\n",
    "            num_s = len(self.states)\n",
    "            num_a = len(self.actions)\n",
    "            T = np.empty([num_s,num_a,num_s])\n",
    "            i = 0\n",
    "            for state, proba_1 in self.transition_proba.items():\n",
    "                j = 0\n",
    "                for action, proba_2 in proba_1.items():\n",
    "                    T[i,j,:] = list(proba_2.values())\n",
    "                    j = j+1\n",
    "                i = i+1\n",
    "            return T\n",
    "    \n",
    "        def get_expected_reward(self):\n",
    "            T = self.get_transition_matrix()\n",
    "            num_s = len(self.states)\n",
    "            num_a = len(self.actions)\n",
    "            R = np.empty([num_s,num_a])\n",
    "            i = 0\n",
    "            for state in self.states:\n",
    "                j = 0\n",
    "                reward_dic = self.reward[state]\n",
    "                for action,rewards in reward_dic.items():\n",
    "                    R[i,j] = np.dot(T[i,j,:],np.array(list(rewards.values())))\n",
    "                    j = j+1\n",
    "                i = i+1  \n",
    "#             R_dict = {}\n",
    "#             for i in range(num_s):\n",
    "#                 rewards = {}\n",
    "#                 for j in range(num_a)\n",
    "#                     rewards[self.actions[j]] = R[i:j]\n",
    "#                 R_dict[self.states[i]] = rewards\n",
    "            return R\n",
    "        \n",
    "\n",
    "        def value_iteration(self):\n",
    "            num_s = len(self.states)\n",
    "            num_a = len(self.actions)\n",
    "            v = np.zeros(num_s)\n",
    "            T = self.get_transition_matrix()\n",
    "            R = self.get_expected_reward()\n",
    "            \n",
    "            iterate = True\n",
    "            \n",
    "            while iterate:\n",
    "                OldValues = v.copy()\n",
    "                \n",
    "                for i in range(num_s):\n",
    "                    v[i] = np.max(R[i,:] + self.gamma*np.dot(T[i,:,:],OldValues))\n",
    "                if np.max(np.abs(v-OldValues)) < self.tol:\n",
    "                    iterate = False\n",
    "            \n",
    "            self.values={}\n",
    "            for i in range(num_s):\n",
    "                self.values[self.states[i]] = v[i]\n",
    "        \n",
    "        def extract_policy(self):\n",
    "            if self.values == None:\n",
    "                print('Do value iteration first!')\n",
    "            else:\n",
    "                num_s = len(self.states)\n",
    "                num_a = len(self.actions)\n",
    "                policy = np.zeros(num_s,num_a)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1],\n",
       "        [1, 1]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [1, 1]],\n",
       "\n",
       "       [[1, 1],\n",
       "        [1, 1]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,1],[1,1]],[[1,1],[1,1]],[[1,1],[1,1]]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.dot(a[0,:,:],[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
